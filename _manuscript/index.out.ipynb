{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cicada Data\n",
    "\n",
    "Data analysis reproduction concerning the cicada’s genome.\n",
    "\n",
    "Eric Mossotti  \n",
    "Jul 15, 2024\n",
    "\n",
    "To reproduce DNA Zoo’s summary table on the 17-year cicada.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Problem\n",
    "\n",
    "The steps involved in reproducing data can be unclear.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "To elaborate on the objective stated at the top of this document, I seek to supplement DNA Zoo’s report with an accessible data analysis (DA) pipeline. To accomplish this, I independently reproduce the original article’s table while documenting every data processing step. Although there’s nothing wrong with the original works, things can always be taken further. ([*DNA Zoo*](#ref-dnazoo)), ([*Little 17-Year Cicada*, 2023](#ref-magicica))\n",
    "\n",
    "### Stakeholders\n",
    "\n",
    "This might be of interest to the original authors of the article. More generally, the spirit of this work could transfer to other domains of data intensive research and analytics.\n",
    "\n",
    "### Source\n",
    "\n",
    "All data used within this report was freely available from a public database hosted by DNA Zoo. ([*Dnazoo.s3*](#ref-dnazoo.s))\n",
    "\n",
    "## Pipeline"
   ],
   "id": "ace5fa41-5b40-4694-b99c-ff5603adf0e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "layout-align": "default"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {
      "text/html": [
       "<pre class=\"mermaid mermaid-js\">\n",
       "flowchart TB\n",
       "    A((1)):::circle --&gt; B((2)):::circle\n",
       "    B --&gt; C((3)):::circle\n",
       "    C --&gt; D((4)):::circle\n",
       "\n",
       "    subgraph Extract [&quot;1. Extract&quot;]\n",
       "        direction LR\n",
       "        A1[&quot;directorize.py&quot;] --&gt; A2[&quot;importer.py&quot;]\n",
       "    end\n",
       "    subgraph Transform [&quot;2. Transform&quot;]\n",
       "      direction TB\n",
       "      B1{&quot;decompress.py&quot;} -.-&gt;|.fasta| B2[&quot;assembly_stats&quot;] -.-&gt;|summary_stats.txt| B3[&quot;assemblyFramer.py&quot;]\n",
       "      B1 -.-&gt;|.fasta| B4[&quot;assemblyDictionary.py&quot;]\n",
       "    end\n",
       "    subgraph Load [&quot;3. Load&quot;]\n",
       "        direction TB\n",
       "        C1{&quot;strint.py&quot;}\n",
       "    end\n",
       "    subgraph Present [&quot;4. Present&quot;]\n",
       "        direction TB\n",
       "        D1[&quot;DNA Zoo&#39;s Table, Reproduced&quot;]\n",
       "    end\n",
       "    \n",
       "    A ~~~ Extract\n",
       "    B ~~~ Transform\n",
       "    C ~~~ Load\n",
       "    D ~~~ Present\n",
       "    \n",
       "    A2 -.-&gt;|fasta.gz| B1\n",
       "    B3 -.-&gt;|dataframe| C1\n",
       "    B4 -.-&gt;|dict| C1\n",
       "    C1 -.-&gt;|strings| Present\n",
       "    C1 -.-&gt;|strings| Present\n",
       "    \n",
       "</pre>"
      ]
     }
    }
   ],
   "source": [
    "flowchart TB\n",
    "    A((1)):::circle --> B((2)):::circle\n",
    "    B --> C((3)):::circle\n",
    "    C --> D((4)):::circle\n",
    "\n",
    "    subgraph Extract [\"1. Extract\"]\n",
    "        direction LR\n",
    "        A1[\"directorize.py\"] --> A2[\"importer.py\"]\n",
    "    end\n",
    "    subgraph Transform [\"2. Transform\"]\n",
    "      direction TB\n",
    "      B1{\"decompress.py\"} -.->|.fasta| B2[\"assembly_stats\"] -.->|summary_stats.txt| B3[\"assemblyFramer.py\"]\n",
    "      B1 -.->|.fasta| B4[\"assemblyDictionary.py\"]\n",
    "    end\n",
    "    subgraph Load [\"3. Load\"]\n",
    "        direction TB\n",
    "        C1{\"strint.py\"}\n",
    "    end\n",
    "    subgraph Present [\"4. Present\"]\n",
    "        direction TB\n",
    "        D1[\"DNA Zoo's Table, Reproduced\"]\n",
    "    end\n",
    "    \n",
    "    A ~~~ Extract\n",
    "    B ~~~ Transform\n",
    "    C ~~~ Load\n",
    "    D ~~~ Present\n",
    "    \n",
    "    A2 -.->|fasta.gz| B1\n",
    "    B3 -.->|dataframe| C1\n",
    "    B4 -.->|dict| C1\n",
    "    C1 -.->|strings| Present\n",
    "    C1 -.->|strings| Present"
   ],
   "id": "802991ee-1265-422c-8f95-5184234c8eeb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Extract\n",
    "\n",
    "This would be the data extraction phase of the DA pipeline.\n",
    "\n",
    "### 1.1 Create Project Directory"
   ],
   "id": "5c8aabea-0eaa-4651-8cae-d68abd2deadc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"00_Extract/scripts/directorize.py\")"
   ],
   "id": "78e503c6-e561-4db7-9a8b-cc79dc3b4ad9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def directorize(base_path, structure):\n",
    "    \n",
    "    for dir_name, subdirs in structure.items():\n",
    "        dir_path = os.path.join(base_path, dir_name)\n",
    "        os.makedirs(dir_path, exist_ok = True)\n",
    "        \n",
    "        for subdir in subdirs:\n",
    "            subdir_path = os.path.join(dir_path, subdir)\n",
    "            os.makedirs(subdir_path, exist_ok = True)"
   ],
   "id": "899ee242-8ae6-4771-8135-049ebda231e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory structure\n",
    "structure = {\n",
    "    \"00_Extract/\": [\"data/\", \"scripts/\"],\n",
    "    \"01_Transform/\": [\"data/\", \"scripts/\"],\n",
    "    \"02_Load/\": [\"data/\", \"scripts/\"],\n",
    "    \"03_Present/\": [\"data/\", \"scripts/\"]\n",
    "}\n",
    "\n",
    "# Create the analysis folder structure in a preferred base directory\n",
    "# \"\" = project's working directory\n",
    "directorize(\"\", structure)"
   ],
   "id": "91411396-4210-484d-94e2-96ec3e8c6fce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download to Local Machine"
   ],
   "id": "a92009b1-3d35-49da-8fe5-d6130a5b3ff1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\n",
    "    \"00_Extract/scripts/importer.py\")"
   ],
   "id": "13855ec0-7e4e-4206-ae7b-b169971dcbc2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Import data from the web with wget\n",
    "import os\n",
    "import sys\n",
    "import wget\n",
    "\n",
    "def importer (fileMap):\n",
    "    # Download from URL to path and notify when complete\n",
    "    for url, file_path in fileMap.items():\n",
    "        # Checking file existence\n",
    "        if not os.path.exists(file_path):\n",
    "            wget.download(url, file_path)\n",
    "            print(f\"{file_path} written\")\n",
    "        else:\n",
    "            print(f\"{file_path} already exists.\")"
   ],
   "id": "b7e606c7-0888-41f0-8345-04080008ff76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "00_Extract/data/magicicada.fasta.gz already exists."
     ]
    }
   ],
   "source": [
    "# Set the url\n",
    "url = \"https://dnazoo.s3.wasabisys.com/Magicicada_septendecula/magicicada_hifiasm.asm.bp.p_ctg_HiC.fasta.gz\"\n",
    "\n",
    "# Set the local file path\n",
    "fpath = \"00_Extract/data/magicicada.fasta.gz\"\n",
    "\n",
    "# Map the url to the file path\n",
    "fileMap = {url: fpath}\n",
    "\n",
    "importer(fileMap)"
   ],
   "id": "a57a921a-fef7-4b64-ac50-9cd23eeb9ef5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The specific link used to download all data from**\n",
    ">\n",
    "> <https://dnazoo.s3.wasabisys.com/Magicicada_septendecula/magicicada_hifiasm.asm.bp.p_ctg_HiC.fasta.gz>\n",
    "\n",
    "## 2 Transform\n",
    "\n",
    "The data transformation phase of the pipeline.\n",
    "\n",
    "### 2.1 Decompress .GZ"
   ],
   "id": "ac57f584-52bd-4f82-9383-12527e626c3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"01_Transform/scripts/decompress.py\")"
   ],
   "id": "e39c57a4-dbfc-4765-b63c-aa7ea3059bbc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Decompress the gz file with gzip\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def decompress(gzFasta, fasta):\n",
    "    \n",
    "    # If not decompressed, then decompress and redirect to a new file path\n",
    "    if not os.path.exists(fasta):\n",
    "        # File doesn't exist, then decompress\n",
    "        with gzip.open(gzFasta, 'rb') as f_in:\n",
    "            with open(fasta, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"{fasta} has been decompressed and written.\")\n",
    "    else:\n",
    "        print(f\"The file {fasta} already exists. Skipping unzip.\")"
   ],
   "id": "5cdbce92-99a1-41de-92b1-23c79311e95e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The file 01_Transform/data/magicicada.fasta already exists. Skipping unzip."
     ]
    }
   ],
   "source": [
    "# Set the compressed fasta.gz file variable\n",
    "gzFasta = \"00_Extract/data/magicicada.fasta.gz\"\n",
    "\n",
    "# Set the decompressed fasta file variable\n",
    "fasta = \"01_Transform/data/magicicada.fasta\"\n",
    "\n",
    "# Pass file paths to the function\n",
    "decompress(gzFasta, fasta)"
   ],
   "id": "8d92daaa-105c-4f76-a337-5c0528b72770"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fasta to Text, to DataFrame"
   ],
   "id": "6205dad3-629f-44a9-a78b-08797792536e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"03_Present/scripts/formaFrame.py\")"
   ],
   "id": "d33f4dea-6fed-4c73-9288-62d3375d16e5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chunk should be ran locally instead of with `quarto render`. When working with the source file, change the code-chunk language specifier from `{.bash}` back to `{bash}`. You might have to add the `{bash}` tag entirely back to the div. Not sure how else to go about accomplishing this within my current Quarto project setup. ([Trizna, 2020](#ref-trizna2020))\n",
    "\n",
    "``` bash\n",
    "# BASH SCRIPT\n",
    "\n",
    "# The uncompressed fasta file variable\n",
    "fasta=01_Transform/data/magicicada.fasta\n",
    "\n",
    "# The text file path variable generated by the script\n",
    "summary_stats=01_Transform/data/summary_stats.txt\n",
    "\n",
    "assembly_stats $fasta > $summary_stats\n",
    "```\n",
    "\n",
    "Transform the text file into a Python dataframe. I am opting not to blanket change data-types as output format could vary by user preference."
   ],
   "id": "00a14e93-f4ff-4197-8686-3cf10ccebbdf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import  external python script to local library environment\n",
    "reticulate::source_python(\"01_Transform/scripts/assemblyFramer.py\")"
   ],
   "id": "a75fc8cb-2621-419b-a791-4478cb50579d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\"\"\" Utilizes Python string methods and multi-indexing \n",
    "to process assembly_stats' output text file \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def assemblyFramer(statsPath = None):\n",
    "    \n",
    "    #---- Read Text File\n",
    "    with open(statsPath, 'r') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "    #---- Regex Matching\n",
    "    pairs = re.findall(r\"\\\"\\w+\\\"\\:\\s\\d*\\.?\\d*\", content)\n",
    "    \n",
    "    #---- Clean Strings\n",
    "    cleaned_list = [pair.replace('\"', '').replace(':', '').strip() for pair in pairs]\n",
    "    \n",
    "    #---- Split Strings\n",
    "    labeled_list = [item.split() for item in cleaned_list]\n",
    "    \n",
    "    #---- Create DataFrame\n",
    "    df = pd.DataFrame(labeled_list, columns = ['Label', 'Value'])\n",
    "    \n",
    "    #---- Add Category Column\n",
    "    df['Category'] = ['Contigs'] * 17 + ['Scaffolds'] * 17\n",
    "    \n",
    "    #---- Create Arrays\n",
    "    category_array = pd.Series.to_list(df['Category'])\n",
    "    label_array = pd.Series.to_list(df['Label'])\n",
    "    value_array = pd.Series.to_list(df['Value'])\n",
    "    \n",
    "    #---- Combine Arrays to List\n",
    "    arrayList = [category_array, label_array]\n",
    "    \n",
    "    #---- Define Multi-Level Indices\n",
    "    indices = pd.MultiIndex.from_arrays(arrays = arrayList, names = ('Category', 'Label'))\n",
    "    \n",
    "    #---- Index a DataFrame \n",
    "    df_indexed = pd.DataFrame(data = value_array, index = indices)\n",
    "    \n",
    "    #---- Rename Non-Indexed Column\n",
    "    df_indexed = df_indexed.rename(columns = {0:\"Value\"})\n",
    "    \n",
    "    return df_indexed\n",
    "```"
   ],
   "id": "e2f3f298-28e4-45b0-ba51-f9a2cac30a73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the local text file path\n",
    "statsPath = \"01_Transform/data/summary_stats.txt\"\n",
    "# Run to yield an multi-indexed dataframe\n",
    "df = assemblyFramer(statsPath)"
   ],
   "id": "dc5a8b1c-abed-404c-9101-5ea71798ba6c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Fasta to Dictionary"
   ],
   "id": "a94228d3-b899-45fd-b453-f88536ad4dc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"01_Transform/scripts/assemblyDictionary.py\")"
   ],
   "id": "9ef7095f-ff8d-4a66-a907-c16d055cdd63"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\"\"\"\n",
    "Parsing genomic data in a memory-efficent way by not loading the entire\n",
    "file into memory at once. The file is processed one line at a time, grouping\n",
    "related lines together. \n",
    "\n",
    "Statistics such as N50 are crucial in assessing the contiguity \n",
    "of a genome assembly, with higher N50 values generally indicating \n",
    "a more contiguous assembly.\n",
    "\n",
    "#----\n",
    "read_genome()\n",
    "____________\n",
    "\n",
    "  1. Differentiate between scaffolds (which may contain gaps) and contigs \n",
    "     (continuous sequences). \n",
    "  \n",
    "    1a. Calculate the GC content, which is an important genomic \n",
    "        characteristic.\n",
    "\n",
    "    1b. Prepare lists of contig and scaffold lengths for further statistical \n",
    "        analysis.\n",
    "\n",
    "The distinction between contigs and scaffolds is important in genome assembly, \n",
    "as it provides information about the continuity and completeness of \n",
    "the assembly.\n",
    "\n",
    "\n",
    "\n",
    "#----\n",
    "fasta_iter()\n",
    "____________\n",
    "\n",
    "Groups the .fasta file data into alternating groups of headers and sequences.\n",
    "It is a generator function that will pause until the next item is requested \n",
    "after yielding a tuple.\n",
    "\n",
    "The iterator groups two aspects:\n",
    "  a. Single header lines starting with '>'\n",
    "  b. Subsequent lines until the next '>'\n",
    "\n",
    "#----\n",
    "calculate_stats()\n",
    "_________________\n",
    "\n",
    "Processes an array of sequence lengths which: \n",
    "\n",
    "  1. Differentiates between scaffolds (which may contain gaps) and contigs \n",
    "     (continuous sequences). \n",
    "     \n",
    "  2. Prepares lists of contig and scaffold lengths for further statistical \n",
    "     analysis.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "\n",
    "#---- fasta_iter()\n",
    "def fasta_iter(fasta_file):\n",
    "    \n",
    "    fh = open(fasta_file)\n",
    "    \n",
    "    # Only need the second part, or code sequences part, of the grouped by items\n",
    "    fa_iter = (x[1] for x in groupby(fh, lambda line: line[0] == \">\"))\n",
    "    \n",
    "    for header in fa_iter:\n",
    "        \n",
    "        # Get first line of group; drop the \">\" and starting/trailing whitespace\n",
    "        header = next(header)[1:].strip()\n",
    "        \n",
    "        # Join all sequence lines to one string; conv to uppercase; remv whitespace\n",
    "        seq = \"\".join(s.upper().strip() for s in next(fa_iter))\n",
    "        \n",
    "        yield header, seq\n",
    "\n",
    "\n",
    "#---- read_genome\n",
    "def read_genome(fasta_file):\n",
    "    \n",
    "    gc = 0\n",
    "    total_len = 0\n",
    "    \n",
    "    contig_lens = []\n",
    "    scaffold_lens = []\n",
    "    \n",
    "    # Ignore header information (the '_' part) and process sequence data\n",
    "    for _, seq in fasta_iter(fasta_file):\n",
    "        \n",
    "        # Add sequence (scaffold) length\n",
    "        scaffold_lens.append(len(seq))\n",
    "        # NN reprs gaps in scaffold, which are contigs\n",
    "        if \"NN\" in seq:\n",
    "            # Add split sequences to contig list if gap\n",
    "            contig_list = seq.split(\"NN\")\n",
    "            \n",
    "        else:\n",
    "            # Add sequence to contig list\n",
    "            contig_list = [seq]\n",
    "            \n",
    "        for contig in contig_list:\n",
    "            # An initial check for 0-length contigs\n",
    "            if len(contig):\n",
    "              gc += contig.count('G') + contig.count('C')\n",
    "              # Update the total length\n",
    "              total_len += len(contig)\n",
    "              # Add  length to list of contig lengths\n",
    "              contig_lens.append(len(contig))\n",
    "    \n",
    "    gc_cont = (gc / total_len) * 100\n",
    "\n",
    "    return contig_lens, scaffold_lens, gc_cont\n",
    "\n",
    "#---- calculate_stats()\n",
    "def calculate_stats(seq_lens, gc_cont):\n",
    "    \n",
    "    # Empty dictionary\n",
    "    stats = {}\n",
    "    # The set of sequence lengths are converted to a NumPy array\n",
    "    seq_array = np.array(seq_lens)\n",
    "    \n",
    "    # Counts the individual sequences\n",
    "    stats['sequence_count'] = seq_array.size\n",
    "    \n",
    "    stats['gc_content'] = gc_cont\n",
    "\n",
    "    # Sort lengths by descending order\n",
    "    sorted_lens = seq_array[np.argsort(-seq_array)]\n",
    "    \n",
    "    # The first length is the longest due to sorting\n",
    "    stats['longest'] = int(sorted_lens[0])\n",
    "    \n",
    "    # Likewise, shortest length is at the end\n",
    "    stats['shortest'] = int(sorted_lens[-1])\n",
    "    \n",
    "    stats['median'] = np.median(sorted_lens)\n",
    "    \n",
    "    stats['mean'] = np.mean(sorted_lens)\n",
    "    \n",
    "    # Sums the total length of all sequences\n",
    "    stats['total_bps'] = int(np.sum(sorted_lens))\n",
    "    \n",
    "    # An array of cumulative sums to calculate N50 efficiently\n",
    "    csum = np.cumsum(sorted_lens)\n",
    "    \n",
    "    for level in [10, 20, 30, 40, 50]:\n",
    "        \n",
    "        # Calculate target base pair count for level\n",
    "        nx = int(stats['total_bps'] * (level / 100))\n",
    "        \n",
    "        # Find smallest bp value in array, >= to the target %\n",
    "        csumn = min(csum[csum >= nx])\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        --- The original code in the next line:\n",
    "  \n",
    "          l_level = int(np.where(csum == csumn)[0])\n",
    "          \n",
    "        --- Has been changed to:\n",
    "            \n",
    "          l_level = int(np.where(csum == csumn)[0][0])\n",
    "\n",
    "        This finds the index where the cumulative sum equals csumn, which \n",
    "        represents the number of sequences needed to reach the target \n",
    "        percentage of base pairs. \n",
    "        \n",
    "        I've added an extra [0] to fix the NumPy deprecation warning. This \n",
    "        ensures return of a scalar value from the array, as the extra '[0]' \n",
    "        accesses the first element of the first array.\n",
    "        \n",
    "        The console warning (in Rstudio) that's now been resolved: \n",
    "        \n",
    "          'Conversion of an array with ndim > 0 to a scalar is deprecated, and \n",
    "          will error in future. Ensure you extract a single element from your \n",
    "          array before performing this operation. (Deprecated NumPy 1.25.)'\n",
    "          \n",
    "        \"\"\"\n",
    "        \n",
    "        # Determine the index where seqs required to reach target % of bps is met\n",
    "        l_level = int(np.where(csum == csumn)[0][0])\n",
    "        \n",
    "        # Get bp length of seq at index, l_level, for the N-statistic value\n",
    "        n_level = int(sorted_lens[l_level])\n",
    "        \n",
    "        stats['L' + str(level)] = l_level\n",
    "\n",
    "        # Store the statistic in dictionary, mapped to its name key\n",
    "        stats['N' + str(level)] = n_level\n",
    "        \n",
    "    return stats\n",
    "\n",
    "\n",
    "#---- assemblyDictionary\n",
    "def assemblyDictionary(infilename):\n",
    "    \n",
    "    # Return two np arrays of lengths\n",
    "    contig_lens, scaffold_lens, gc_cont = read_genome(infilename)\n",
    "    \n",
    "    # Return contig stats from contig lengths\n",
    "    contig_stats = calculate_stats(contig_lens, gc_cont)\n",
    "    \n",
    "    # Return scaffold stats from scaffold lengths\n",
    "    scaffold_stats = calculate_stats(scaffold_lens, gc_cont)\n",
    "    \n",
    "    # A dictionary of outputs is easily queried.\n",
    "    stat_output = {'Contig Stats': contig_stats,\n",
    "                   'Scaffold Stats': scaffold_stats}\n",
    "    \n",
    "    return stat_output\n",
    "```"
   ],
   "id": "d41ce6e5-bc2b-4408-9506-9bd76dcd4b30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"02_Load/scripts/strint.py\")"
   ],
   "id": "6954f157-fe38-4387-8645-173f22d7ba88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsDict = assemblyDictionary(\"01_Transform/data/magicicada.fasta\")"
   ],
   "id": "52e9d17a-b7e2-47a5-a57d-1451a581020a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Load\n",
    "\n",
    "This is the data loading phase. Following completion of this stage, querying the data should be more intuitive than before.\n",
    "\n",
    "### 3.1 Assign Variables with `strint.py`\n",
    "\n",
    "``` python\n",
    "\"\"\" \n",
    "Formats the string representation of an \n",
    "integer value as a comma separated string \n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def strint(data, category, label):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # Existing DataFrame handling code\n",
    "        stat = data.loc[(category, label), \"Value\"]\n",
    "        \n",
    "        # Set boolean match value\n",
    "        isFloat = re.search(r\"\\.\", str(stat))\n",
    "        \n",
    "        # Convert to float if there is a decimal\n",
    "        if isFloat:\n",
    "            stat = pd.to_numeric(stat, downcast=\"float\")\n",
    "        else:\n",
    "            # Else convert to an integer \n",
    "            stat = pd.to_numeric(stat, downcast=\"integer\")\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        # New dictionary handling code\n",
    "        #stat = data.get(f\"{category} {label}\")\n",
    "        stat = data[category][label]\n",
    "        \n",
    "        if stat is None:\n",
    "            raise KeyError(f\"Key '{category} {label}' not found in the dictionary\")\n",
    "        \n",
    "        # No need to convert to numeric as values are already integers or floats\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(\"Input must be a pandas DataFrame or a dictionary\")\n",
    "\n",
    "    # Add a thousands separator and convert back to a string\n",
    "    stat = f'{stat:,}'\n",
    "    \n",
    "    return stat\n",
    "```\n",
    "\n",
    "### 3.2 DataFrame Input"
   ],
   "id": "c0e8291d-3412-4755-a472-92683f424a5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Contigs\n",
    "ctig_len = strint(df, \"Contigs\", \"total_bps\")\n",
    "ctig_count = strint(df, \"Contigs\", \"sequence_count\")\n",
    "ctig_n50 = strint(df, \"Contigs\", \"N50\")\n",
    "ctig_max = strint(df, \"Contigs\", \"longest\")\n",
    "\n",
    "#---- Scaffolds\n",
    "sfld_len = strint(df, \"Scaffolds\", \"total_bps\")\n",
    "sfld_count = strint(df, \"Scaffolds\", \"sequence_count\")\n",
    "sfld_n50 = strint(df, \"Scaffolds\", \"N50\")\n",
    "sfld_max = strint(df, \"Scaffolds\", \"longest\")"
   ],
   "id": "2e7382ca-b2f4-4861-b653-bc9e33a38e4c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Multi-index dataframe query syntax**\n",
    ">\n",
    "> ``` python\n",
    "> \"\"\"\n",
    ">\n",
    "> strint(dataframe, category, label)\n",
    ">\n",
    "> category options\n",
    "> ----------------\n",
    ">   Contigs\n",
    ">   Scaffolds\n",
    ">\n",
    ">\n",
    "> label options\n",
    "> ----------------\n",
    ">   L10\n",
    ">   L20\n",
    ">   L30\n",
    ">   L40\n",
    ">   L50\n",
    ">   N10\n",
    ">   N20\n",
    ">   N30\n",
    ">   N40\n",
    ">   N50\n",
    ">   gc_content\n",
    ">   longest\n",
    ">   mean\n",
    ">   median\n",
    ">   sequence_count\n",
    ">   shortest\n",
    ">   total_bps\n",
    ">   \n",
    "> \"\"\"\n",
    ">\n",
    "> ctig_len = strint(df, \"Contigs\", \"N50\")\n",
    ">\n",
    "> # -->> Looking inside the strint(dataframe, category, label) function ---->>\n",
    ">\n",
    "> # Which then finds the desired value or 'Value'\n",
    "> stat = dataframe.loc[(category, label), \"Value\"]\n",
    "> ```\n",
    "\n",
    "### 3.3 Dictionary Input"
   ],
   "id": "f68b9f5d-0334-47d2-8434-ed47a9196cd8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Contigs\n",
    "ctig_len = strint(statsDict, \"Contig Stats\", \"total_bps\")\n",
    "ctig_count = strint(statsDict, \"Contig Stats\", \"sequence_count\")\n",
    "ctig_n50 = strint(statsDict, \"Contig Stats\", \"N50\")\n",
    "ctig_max = strint(statsDict, \"Contig Stats\", \"longest\")\n",
    "\n",
    "#---- Scaffolds\n",
    "sfld_len = strint(statsDict, \"Scaffold Stats\", \"total_bps\")\n",
    "sfld_count = strint(statsDict, \"Scaffold Stats\", \"sequence_count\")\n",
    "sfld_n50 = strint(statsDict, \"Scaffold Stats\", \"N50\")\n",
    "sfld_max = strint(statsDict, \"Scaffold Stats\", \"longest\")"
   ],
   "id": "2c92b110-8935-4f18-be7e-1e3981a0431e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dictionary query syntax**\n",
    ">\n",
    "> ``` python\n",
    "> \"\"\"\n",
    ">\n",
    "> strint(dataframe, category, label)\n",
    ">\n",
    "> category options\n",
    "> ----------------\n",
    ">   Contig Stats\n",
    ">   Scaffold Stats\n",
    ">\n",
    ">\n",
    "> label options\n",
    "> ----------------\n",
    ">   L10\n",
    ">   L20\n",
    ">   L30\n",
    ">   L40\n",
    ">   L50\n",
    ">   N10\n",
    ">   N20\n",
    ">   N30\n",
    ">   N40\n",
    ">   N50\n",
    ">   gc_content\n",
    ">   longest\n",
    ">   mean\n",
    ">   median\n",
    ">   sequence_count\n",
    ">   shortest\n",
    ">   total_bps\n",
    ">   \n",
    "> \"\"\"\n",
    ">\n",
    "> # A look inside assemblyDictionary.py where stat_output is returned\n",
    "> stat_output = {'Contig Stats': contig_stats,\n",
    ">                'Scaffold Stats': scaffold_stats}\n",
    ">\n",
    "> ctig_len = statsDict[\"Contig Stats\"][\"total_bps\"]\n",
    "> ```\n",
    "\n",
    "## 4 Present\n",
    "\n",
    "### 4.1 The Pandas Table\n",
    "\n",
    "This is a slightly formatted view of the Pandas table designed to be more easily queried to return the desired statistic. If, however, you’d like to treat the Styler object as the unchanged, dataframe object, use the `forma_df.data` syntax.\n",
    "\n",
    "[:The original dataframe output:](#NutFrame)"
   ],
   "id": "3423ca42-9ee3-4348-95d6-ae0ceeaf312e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n"
      ]
     }
    }
   ],
   "source": [
    "# Display with Style \n",
    "forma_df = df.loc[:].style.pipe(formaFrame)\n",
    "\n",
    "forma_df"
   ],
   "id": "cff2bb05-1604-47e5-a4f0-2d2d2f4cdd09"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 DNA Zoo’s Table, Reproduced"
   ],
   "id": "08572e56-c791-49f4-8454-64c0b28a4595"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(reticulate)"
   ],
   "id": "4df34229-5790-4d81-adbe-2a7dbff4a78f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the library makes it simpler for inserting the values into the table below. For example, I would have had to type 4,902,968, but now, I only need to type 4,902,968 into the individual cells. I needed to convert the Python into R objects, as the knitr engine used in rendering this document does not seem to display output from execution of inline Python code directly.\n",
    "\n",
    "|  |  |  |  |\n",
    "|------------------|------------------|------------------|-------------------|\n",
    "| **Contig length (bp)** | **Number of contigs** | **Contig N50 (bp)** | **Longest contig (bp)** |\n",
    "| 6,520,445,364 | 4,200 | 4,902,968 | 43,529,772 |\n",
    "| **Scaffold length (bp)** | **Number of scaffolds** | **Scaffold N50 (bp)** | **Longest scaffold (bp)** |\n",
    "| 6,521,530,364 | 2,030 | 518,932,092 | 1,438,277,616 |\n",
    "\n",
    "## :x NutFrame"
   ],
   "id": "a37caead-c917-4682-b34a-e01659b01749"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                       Value\n",
      "Category  Label                             \n",
      "Contigs   L10                             41\n",
      "          L20                             99\n",
      "          L30                            174\n",
      "          L40                            267\n",
      "          L50                            385\n",
      "          N10                       12643769\n",
      "          N20                        9681846\n",
      "          N30                        7895799\n",
      "          N40                        6288966\n",
      "          N50                        4902968\n",
      "          gc_content      35.248103813419206\n",
      "          longest                   43529772\n",
      "          mean            1552486.9914285715\n",
      "          median                    331935.0\n",
      "          sequence_count                4200\n",
      "          shortest                      1000\n",
      "          total_bps               6520445364\n",
      "Scaffolds L10                              0\n",
      "          L20                              0\n",
      "          L30                              1\n",
      "          L40                              2\n",
      "          L50                              3\n",
      "          N10                     1438277616\n",
      "          N20                     1438277616\n",
      "          N30                      915491830\n",
      "          N40                      607508155\n",
      "          N50                      518932092\n",
      "          gc_content      35.248103813419206\n",
      "          longest                 1438277616\n",
      "          mean             3212576.533990148\n",
      "          median                     62362.5\n",
      "          sequence_count                2030\n",
      "          shortest                      1000\n",
      "          total_bps               6521530364"
     ]
    }
   ],
   "source": [
    "# The un-styled dataframe output\n",
    "forma_df.data"
   ],
   "id": "a80215f1-4250-4a80-a648-c96f4d371590"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DNA Zoo*. <https://www.dnazoo.org>\n",
    "\n",
    "*Dnazoo.s3*. <https://dnazoo.s3.wasabisys.com/index.html?prefix=Magicicada_septendecula/>\n",
    "\n",
    "*Little 17-year cicada*. (2023). <https://www.dnazoo.org/assemblies/magicicada_septendecula>\n",
    "\n",
    "Trizna, M. (2020). *Assembly\\_stats 0.1.4*. Zenodo. <https://doi.org/10.5281/ZENODO.3968774>"
   ],
   "id": "a6c1a569-105c-469c-be35-84a54c6ed20c"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
