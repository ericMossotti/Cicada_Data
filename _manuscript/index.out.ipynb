{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cicada Data\n",
    "\n",
    "Data analysis reproduction concerning the cicada’s genome.\n",
    "\n",
    "Eric Mossotti  \n",
    "Jul 15, 2024\n",
    "\n",
    "To reproduce DNA Zoo’s summary table on the 17-year cicada.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "[The full repo for this project with instructions on how to replicate and run with near verbatim output.](https://github.com/ericMossotti/Cicada_Data)\n",
    "\n",
    "### Problem\n",
    "\n",
    "The steps involved in reproducing data can be unclear.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "To elaborate on the objective stated at the top of this document, I seek to supplement DNA Zoo’s report with an accessible data analysis (DA) pipeline. To accomplish this, I independently reproduce the original article’s table while documenting every data processing step. Although there’s nothing wrong with the original works, things can always be taken further. ([*DNA Zoo*](#ref-dnazoo)), ([*Little 17-Year Cicada*, 2023](#ref-magicica))\n",
    "\n",
    "### Stakeholders\n",
    "\n",
    "This might be of interest to the original authors of the article. More generally, the spirit of this work could transfer to other domains of data intensive research and analytics.\n",
    "\n",
    "### Source\n",
    "\n",
    "All data used within this report was freely available from a public database hosted by DNA Zoo. ([*Dnazoo.s3*](#ref-dnazoo.s))\n",
    "\n",
    "### Pipeline"
   ],
   "id": "86e32a38-3389-4638-b184-7563e61c4a89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "layout-align": "default"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {
      "text/html": [
       "<pre class=\"mermaid mermaid-js\">\n",
       "flowchart TB\n",
       "    A((1)):::circle --&gt; B((2)):::circle\n",
       "    B --&gt; C((3)):::circle\n",
       "    C --&gt; D((4)):::circle\n",
       "\n",
       "    subgraph Extract [&quot;1. Extract&quot;]\n",
       "        direction LR\n",
       "        A1[&quot;directorize.py&quot;] --&gt; A2[&quot;importer.py&quot;]\n",
       "    end\n",
       "    subgraph Transform [&quot;2. Transform&quot;]\n",
       "      direction TB\n",
       "      B1{&quot;decompress.py&quot;} -.-&gt;|.fasta| B2[&quot;assembly_stats&quot;] -.-&gt;|summary_stats.txt| B3[&quot;assemblyFramer.py&quot;]\n",
       "      B1 -.-&gt;|.fasta| B4[&quot;assemblyDictionary.py&quot;]\n",
       "    end\n",
       "    subgraph Load [&quot;3. Load&quot;]\n",
       "        direction TB\n",
       "        C1{&quot;strint.py&quot;}\n",
       "    end\n",
       "    subgraph Present [&quot;4. Present&quot;]\n",
       "        direction TB\n",
       "        D1[&quot;DNA Zoo&#39;s Table, Reproduced&quot;]\n",
       "    end\n",
       "    \n",
       "    A ~~~ Extract\n",
       "    B ~~~ Transform\n",
       "    C ~~~ Load\n",
       "    D ~~~ Present\n",
       "    \n",
       "    A2 -.-&gt;|fasta.gz| B1\n",
       "    B3 -.-&gt;|dataframe| C1\n",
       "    B4 -.-&gt;|dict| C1\n",
       "    C1 -.-&gt;|strings| Present\n",
       "    C1 -.-&gt;|strings| Present\n",
       "    \n",
       "</pre>"
      ]
     }
    }
   ],
   "source": [
    "flowchart TB\n",
    "    A((1)):::circle --> B((2)):::circle\n",
    "    B --> C((3)):::circle\n",
    "    C --> D((4)):::circle\n",
    "\n",
    "    subgraph Extract [\"1. Extract\"]\n",
    "        direction LR\n",
    "        A1[\"directorize.py\"] --> A2[\"importer.py\"]\n",
    "    end\n",
    "    subgraph Transform [\"2. Transform\"]\n",
    "      direction TB\n",
    "      B1{\"decompress.py\"} -.->|.fasta| B2[\"assembly_stats\"] -.->|summary_stats.txt| B3[\"assemblyFramer.py\"]\n",
    "      B1 -.->|.fasta| B4[\"assemblyDictionary.py\"]\n",
    "    end\n",
    "    subgraph Load [\"3. Load\"]\n",
    "        direction TB\n",
    "        C1{\"strint.py\"}\n",
    "    end\n",
    "    subgraph Present [\"4. Present\"]\n",
    "        direction TB\n",
    "        D1[\"DNA Zoo's Table, Reproduced\"]\n",
    "    end\n",
    "    \n",
    "    A ~~~ Extract\n",
    "    B ~~~ Transform\n",
    "    C ~~~ Load\n",
    "    D ~~~ Present\n",
    "    \n",
    "    A2 -.->|fasta.gz| B1\n",
    "    B3 -.->|dataframe| C1\n",
    "    B4 -.->|dict| C1\n",
    "    C1 -.->|strings| Present\n",
    "    C1 -.->|strings| Present"
   ],
   "id": "e9c48add-4191-43cd-b647-f8f2fad08869"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Extract\n",
    "\n",
    "This would be the data extraction phase of the DA pipeline.\n",
    "\n",
    "### 1.1 Create Project Directory"
   ],
   "id": "47ae27d1-2cd2-46e0-9129-551c6b637f01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automates creation of the DA pipeline directory needed for this project to avoid confusion for those replicating this project, if desired.\n",
    "\n",
    "import os\n",
    "\n",
    "def directorize(base_path, structure):\n",
    "    # Nested directories\n",
    "    for dir_name, subdirs in structure.items():\n",
    "        dir_path = os.path.join(base_path, dir_name)\n",
    "        os.makedirs(dir_path, exist_ok = True)\n",
    "\n",
    "        for subdir in subdirs:\n",
    "            subdir_path = os.path.join(dir_path, subdir)\n",
    "            os.makedirs(subdir_path, exist_ok = True)"
   ],
   "id": "cc5f8586-4a02-43ab-a0c2-5cb6243b4cef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory structure\n",
    "structure = {\n",
    "    \"01_Extract/\": [\"data/\", \"scripts/\"],\n",
    "    \"02_Transform/\": [\"data/\", \"scripts/\"],\n",
    "    \"03_Load/\": [\"data/\", \"scripts/\"],\n",
    "    \"04_Present/\": [\"data/\", \"scripts/\"]\n",
    "}\n",
    "\n",
    "# Create the analysis folder structure in a preferred base directory\n",
    "# \"\" = project's working directory\n",
    "directorize(\"\", structure)"
   ],
   "id": "79f59f60-1a17-4912-ac58-6ed8d956ba34"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download to Local Machine\n",
    "\n",
    "If you want the exact verbatim output, then delete the cloned folder structure (01_Extract through the 04_Present and run this code)."
   ],
   "id": "ebbd70c2-6610-4af9-999e-f0dc896b6dd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Import data from the web with wget\n",
    "import os\n",
    "import sys\n",
    "import wget\n",
    "\n",
    "def importer (fileMap):\n",
    "    # Download from URL to path and notify when complete\n",
    "    for url, file_path in fileMap.items():\n",
    "        # Checking file existence\n",
    "        if not os.path.exists(file_path):\n",
    "            wget.download(url, file_path)\n",
    "            print(f\"{file_path} written\")\n",
    "        else:\n",
    "            print(f\"{file_path} already exists.\")"
   ],
   "id": "b9a11e28-7abc-418f-bc9d-72abd5a1e400"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "01_Extract/data/magicicada.fasta.gz already exists."
     ]
    }
   ],
   "source": [
    "# Set the url\n",
    "fasta_URL = \"https://dnazoo.s3.wasabisys.com/Magicicada_septendecula/magicicada_hifiasm.asm.bp.p_ctg_HiC.fasta.gz\"\n",
    "# Set the desired local file path\n",
    "fasta_PATH = \"01_Extract/data/magicicada.fasta.gz\"\n",
    "\n",
    "#decompress_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/02_Transform/scripts/decompress.py\"\n",
    "#decompress_PATH = \"02_Transform/scripts/decompress.py\"\n",
    "\n",
    "#asmblydict_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/02_Transform/scripts/assemblyDictionary.py\"\n",
    "#asmblydict_PATH = \"02_Transform/scripts/assemblyDictionary.py\"\n",
    "\n",
    "#asmblyframe_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/02_Transform/scripts/assemblyFramer.py\"\n",
    "#asmblyframe_PATH = \"02_Transform/scripts/assemblyFramer.py\"\n",
    "\n",
    "#strint_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/03_Load/scripts/strint.py\"\n",
    "#strint_PATH = \"03_Load/scripts/strint.py\"\n",
    "\n",
    "#formaframe_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/04_Present/scripts/formaFrame.py\"\n",
    "#formaframe_PATH = \"04_Present/scripts/formaFrame.py\"\n",
    "\n",
    "#extractIgnore_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/01_Extract/data/.gitignore\"\n",
    "#extractIgnore_PATH = \"01_Extract/data/.gitignore\"\n",
    "\n",
    "#transformIgnore_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/02_Transform/data/.gitignore\"\n",
    "#transformIgnore_PATH = \"02_Transform/data/.gitignore\"\n",
    "\n",
    "\n",
    "# Map the url to the file path\n",
    "fileMap = {\n",
    "  fasta_URL: fasta_PATH,\n",
    "#  decompress_URL: decompress_PATH,\n",
    " # asmblydict_URL: asmblydict_PATH,\n",
    " # asmblyframe_URL: asmblyframe_PATH,\n",
    " # strint_URL: strint_PATH,\n",
    " # formaframe_URL: formaframe_PATH,\n",
    " # extractIgnore_URL: extractIgnore_PATH,\n",
    " # transformIgnore_URL: transformIgnore_PATH\n",
    "  }\n",
    "\n",
    "importer(fileMap)"
   ],
   "id": "24ba95e1-e27e-4b60-b04a-0fa75984b0b7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The specific link used to download all data from**\n",
    ">\n",
    "> <https://dnazoo.s3.wasabisys.com/Magicicada_septendecula/magicicada_hifiasm.asm.bp.p_ctg_HiC.fasta.gz>\n",
    "\n",
    "## 2 Transform\n",
    "\n",
    "The data transformation phase of the pipeline.\n",
    "\n",
    "### 2.1 Decompress .GZ"
   ],
   "id": "9d098fcc-07bb-4c18-9b9b-a75ce99b1744"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"02_Transform/scripts/decompress.py\")"
   ],
   "id": "2029ff3a-e800-446b-9f09-3c5b3722ca88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Decompress the gz file with gzip\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def decompress(gzFasta, fasta):\n",
    "    \n",
    "    # If not decompressed, then decompress and redirect to a new file path\n",
    "    if not os.path.exists(fasta):\n",
    "        # File doesn't exist, then decompress\n",
    "        with gzip.open(gzFasta, 'rb') as f_in:\n",
    "            with open(fasta, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"{fasta} has been decompressed and written.\")\n",
    "    else:\n",
    "        print(f\"The file {fasta} already exists. Skipping unzip.\")"
   ],
   "id": "4173aac1-595d-4a9d-9d8d-84e00514e90b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The file 02_Transform/data/magicicada.fasta already exists. Skipping unzip."
     ]
    }
   ],
   "source": [
    "# Set the compressed fasta.gz file variable\n",
    "gzFasta = \"01_Extract/data/magicicada.fasta.gz\"\n",
    "\n",
    "# Set the decompressed fasta file variable\n",
    "fasta = \"02_Transform/data/magicicada.fasta\"\n",
    "\n",
    "# Pass file paths to the function\n",
    "decompress(gzFasta, fasta)"
   ],
   "id": "02f0fa5b-f1e2-47bf-90bd-2b8f6001fc49"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 FASTA to Text, to DataFrame\n",
    "\n",
    "Sourcing the dataframe formatting script here, as there is some issue with knitr engine and Python with these parts. Seems like some kind of conflict with switching between dataframe and dictionary data types."
   ],
   "id": "388c88b4-0c5b-4c89-85a9-de80d9e78689"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"04_Present/scripts/formaFrame.py\")"
   ],
   "id": "be9925eb-cfea-4a69-b0fb-8abdda7ca589"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chunk should be ran locally instead of with `quarto render`. When working with the source file, change the code-chunk language specifier from `{.bash}` back to `{bash}`. You might have to add the `{bash}` tag entirely back to the div. Not sure how else to go about accomplishing this within my current Quarto project setup. ([Trizna, 2020](#ref-trizna2020))\n",
    "\n",
    "``` bash\n",
    "# BASH SCRIPT\n",
    "\n",
    "# The uncompressed fasta file variable\n",
    "fasta=02_Transform/data/magicicada.fasta\n",
    "\n",
    "# The text file path variable generated by the script\n",
    "summary_stats=02_Transform/data/summary_stats.txt\n",
    "\n",
    "assembly_stats $fasta > $summary_stats\n",
    "```\n",
    "\n",
    "I need to do this for the rendering part to display the code-cell outputs how I wanted them to. However, you can use the bash code chunk further below and comment this code out if you want. This makes it so you can deactivate the bash code chunk for render but still have everything else work without a hitch if you go simply from empty directory to render with this document."
   ],
   "id": "b1efeb0b-2f75-43f2-8909-fde840c472c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summaryStats_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/02_Transform/data/summary_stats.txt\"\n",
    "\n",
    "#summaryStats_PATH = \"02_Transform/data/summary_stats.txt\"\n",
    "\n",
    "#map_summary = {summaryStats_URL: summaryStats_PATH}\n",
    "\n",
    "#importer(map_summary)"
   ],
   "id": "e07c9613-52d9-4b60-8d53-718934d2d06f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, transform the text file into a Python dataframe. I am opting not to blanket change data-types as output format could vary by user preference."
   ],
   "id": "a19614e3-9600-48ee-8813-14a9a608fdaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import external python script to local library environment\n",
    "reticulate::source_python(\"02_Transform/scripts/assemblyFramer.py\")"
   ],
   "id": "ca52f62c-f3ed-4765-919b-98176396e731"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\"\"\"\n",
    "assemblyFramer.py\n",
    "\n",
    "This script processes a JSON-structured file containing assembly statistics \n",
    "(e.g., contig and scaffold stats) and converts it into a multi-indexed Pandas \n",
    "DataFrame for easier analysis and manipulation.\n",
    "\n",
    "The script is designed to handle file outputs similar to the \n",
    "`summary_stats.json` or `summary_stats.txt` in this project, as long as the \n",
    "content is valid JSON. It extracts key-value pairs from the JSON structure and \n",
    "organizes them into a DataFrame with a hierarchical index (Category and Label).\n",
    "\n",
    "Author: Eric Mossotti\n",
    "Date: 2025-03-12\n",
    "Version: 1.0\n",
    "CC-BY SA\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def assemblyFramer(statsPath = None):\n",
    "    \"\"\"\n",
    "    Reads a JSON-structured assembly stats file and returns a multi-index DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    statsPath : str, optional\n",
    "        The file path to the JSON-structured assembly stats file. Default is None.\n",
    "        The file can have any extension (e.g., `.json`, `.txt`), but the content must be valid JSON.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A Pandas DataFrame with a multi-index (Category, Label) and a single column \"Value\".\n",
    "        The DataFrame contains all key-value pairs extracted from the input JSON file.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> df = assemblyFramer(\"summary_stats.json\")\n",
    "    >>> print(df)\n",
    "                             Value\n",
    "    Category Label                \n",
    "    Contigs  L10                41\n",
    "             L20                99\n",
    "             L30               174\n",
    "             L40               267\n",
    "             L50               385\n",
    "             N10          12643769\n",
    "             N20           9681846\n",
    "             N30           7895799\n",
    "             N40           6288966\n",
    "             N50           4902968\n",
    "             gc_content  35.248104\n",
    "             longest     43529772\n",
    "             mean       1552486.99\n",
    "             median        331935.0\n",
    "             sequence_count   4200\n",
    "             shortest         1000\n",
    "             total_bps   6520445364\n",
    "    Scaffolds L10                 0\n",
    "             L20                 0\n",
    "             L30                 1\n",
    "             L40                 2\n",
    "             L50                 3\n",
    "             N10          1438277616\n",
    "             N20          1438277616\n",
    "             N30           915491830\n",
    "             N40           607508155\n",
    "             N50           518932092\n",
    "             gc_content  35.248104\n",
    "             longest     1438277616\n",
    "             mean       3212576.534\n",
    "             median        62362.5\n",
    "             sequence_count   2030\n",
    "             shortest         1000\n",
    "             total_bps   6521530364\n",
    "    \"\"\"\n",
    "    # Read the JSON file\n",
    "    with open(statsPath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Prepare rows for the DataFrame\n",
    "    rows = []\n",
    "    for category_key, stats in data.items():\n",
    "        # Convert \"Contig Stats\" to \"Contigs\", \"Scaffold Stats\" to \"Scaffolds\"\n",
    "        category = category_key.split()[0] + 's'\n",
    "        for label, value in stats.items():\n",
    "            rows.append({\n",
    "                'Category': category,\n",
    "                'Label': label,\n",
    "                'Value': value\n",
    "            })\n",
    "    \n",
    "    # Create a DataFrame and set multi-index\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.set_index(['Category', 'Label'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "```"
   ],
   "id": "707ae49d-cff5-4807-8612-a0ab6756989d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the local text file path\n",
    "statsPath = \"02_Transform/data/summary_stats.txt\"\n",
    "# Run to yield a multi-indexed dataframe\n",
    "df = assemblyFramer(statsPath)"
   ],
   "id": "d304340f-2679-4cdc-838a-12e21156af17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 FASTA to Dictionary"
   ],
   "id": "5f62406a-cd75-4a19-b6c2-d71640a421cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"02_Transform/scripts/assemblyDictionary.py\")"
   ],
   "id": "b5e60571-b895-431d-ba94-2c660802907b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\"\"\"\n",
    "Parsing genomic data in a memory-efficent way by not loading the entire\n",
    "file into memory at once. The file is processed one line at a time, grouping\n",
    "related lines together. \n",
    "\n",
    "Statistics such as N50 are crucial in assessing the contiguity \n",
    "of a genome assembly, with higher N50 values generally indicating \n",
    "a more contiguous assembly.\n",
    "\n",
    "The distinction between contigs and scaffolds is important in genome assembly, \n",
    "as it provides information about the continuity and completeness of \n",
    "the assembly.\n",
    "\n",
    "\n",
    "#----\n",
    "read_genome()\n",
    "____________\n",
    "\n",
    "Differentiates between scaffolds (which may contain gaps) and contigs \n",
    "(continuous sequences). \n",
    "  \n",
    "Calculate the GC content, which is an important genomic characteristic.\n",
    "Prepare lists of contig and scaffold lengths.\n",
    "\n",
    "\n",
    "#----\n",
    "fasta_iter()\n",
    "____________\n",
    "\n",
    "Groups the .fasta file data into alternating groups of headers and sequences.\n",
    "It is a generator function that will pause until the next item is requested \n",
    "after yielding a tuple.\n",
    "\n",
    "The iterator groups two aspects:\n",
    "  a. Single header lines starting with '>'\n",
    "  b. Subsequent lines until the next '>'\n",
    "\n",
    "\n",
    "#----\n",
    "calculate_stats()\n",
    "_________________\n",
    "\n",
    "Where the stats dictionary values are calculated and keys assigned.\n",
    "\n",
    "\n",
    "#----\n",
    "def assemblyDictionary()\n",
    "________________________\n",
    "\n",
    "Maps the previously created dictionaries of contig and scaffold stats to the \n",
    "category they belong, thereby distinguishing the contigs and scaffold stats. \n",
    "Returning the desired values is quite intuitive as a result.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "\n",
    "#---- fasta_iter()\n",
    "def fasta_iter(fasta_file):\n",
    "    \n",
    "    fh = open(fasta_file)\n",
    "    \n",
    "    # Only need the second part, or code sequences part, of the grouped by items\n",
    "    fa_iter = (x[1] for x in groupby(fh, lambda line: line[0] == \">\"))\n",
    "    \n",
    "    for header in fa_iter:\n",
    "        \n",
    "        # Get first line of group; drop the \">\" and starting/trailing whitespace\n",
    "        header = next(header)[1:].strip()\n",
    "        \n",
    "        # Join all sequence lines to one string; conv to uppercase; remv whitespace\n",
    "        seq = \"\".join(s.upper().strip() for s in next(fa_iter))\n",
    "        \n",
    "        yield header, seq\n",
    "\n",
    "\n",
    "#---- read_genome\n",
    "def read_genome(fasta_file):\n",
    "    \n",
    "    gc = 0\n",
    "    total_len = 0\n",
    "    \n",
    "    contig_lens = []\n",
    "    scaffold_lens = []\n",
    "    \n",
    "    # Ignore header information (the '_' part) and process sequence data\n",
    "    for _, seq in fasta_iter(fasta_file):\n",
    "        \n",
    "        # Add sequence (scaffold) length\n",
    "        scaffold_lens.append(len(seq))\n",
    "        # NN reprs gaps in scaffold, which are contigs\n",
    "        if \"NN\" in seq:\n",
    "            # Add split sequences to contig list if gap\n",
    "            contig_list = seq.split(\"NN\")\n",
    "            \n",
    "        else:\n",
    "            # Add sequence to contig list\n",
    "            contig_list = [seq]\n",
    "            \n",
    "        for contig in contig_list:\n",
    "            # An initial check for 0-length contigs\n",
    "            if len(contig):\n",
    "              gc += contig.count('G') + contig.count('C')\n",
    "              # Update the total length\n",
    "              total_len += len(contig)\n",
    "              # Add  length to list of contig lengths\n",
    "              contig_lens.append(len(contig))\n",
    "    # GC content as the percentage of total genome\n",
    "    gc_cont = (gc / total_len) * 100\n",
    "\n",
    "    return contig_lens, scaffold_lens, gc_cont\n",
    "\n",
    "\n",
    "#---- calculate_stats()\n",
    "def calculate_stats(seq_lens, gc_cont):\n",
    "    \n",
    "    # Empty dictionary\n",
    "    stats = {}\n",
    "    # The set of sequence lengths are converted to a NumPy array\n",
    "    seq_array = np.array(seq_lens)\n",
    "    \n",
    "    # Count the individual sequences\n",
    "    stats['sequence_count'] = seq_array.size\n",
    "    \n",
    "    # GC proportion\n",
    "    stats['gc_content'] = gc_cont\n",
    "\n",
    "    # Sort lengths by descending order\n",
    "    sorted_lens = seq_array[np.argsort(-seq_array)]\n",
    "    \n",
    "    # The first length is the longest due to sorting\n",
    "    stats['longest'] = int(sorted_lens[0])\n",
    "    \n",
    "    # Likewise, shortest length is at the end\n",
    "    stats['shortest'] = int(sorted_lens[-1])\n",
    "    \n",
    "    stats['median'] = np.median(sorted_lens)\n",
    "    \n",
    "    stats['mean'] = np.mean(sorted_lens)\n",
    "    \n",
    "    # Sums the total length of all sequences\n",
    "    stats['total_bps'] = int(np.sum(sorted_lens))\n",
    "    \n",
    "    # An array of cumulative sums to calculate N50 efficiently\n",
    "    csum = np.cumsum(sorted_lens)\n",
    "    \n",
    "    for level in [10, 20, 30, 40, 50]:\n",
    "        \n",
    "        # Calculate target base pair count for the level\n",
    "        nx = int(stats['total_bps'] * (level / 100))\n",
    "        \n",
    "        # Find smallest bp value in array, >= to the target %\n",
    "        csumn = min(csum[csum >= nx])\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        --- The original code in the next line:\n",
    "  \n",
    "          l_level = int(np.where(csum == csumn)[0])\n",
    "          \n",
    "        --- Has been changed to:\n",
    "            \n",
    "          l_level = int(np.where(csum == csumn)[0][0])\n",
    "\n",
    "        This finds the index where the cumulative sum equals csumn, which \n",
    "        represents the number of sequences needed to reach the target \n",
    "        percentage of base pairs. \n",
    "        \n",
    "        I've added an extra [0] to fix the NumPy deprecation warning. This \n",
    "        ensures return of a scalar value from the array, as the extra '[0]' \n",
    "        ensures the first element of the first array is being accessed.\n",
    "        \n",
    "        The console warning (in Rstudio) that's now been resolved: \n",
    "        \n",
    "          'Conversion of an array with ndim > 0 to a scalar is deprecated, and \n",
    "          will error in future. Ensure you extract a single element from your \n",
    "          array before performing this operation. (Deprecated NumPy 1.25.)'\n",
    "          \n",
    "        \"\"\"\n",
    "        \n",
    "        # Determine the index where the sequences required to reach target % of bps is met\n",
    "        l_level = int(np.where(csum == csumn)[0][0])\n",
    "        \n",
    "        # Get bp length of seq at index, l_level, for the N-statistic value\n",
    "        n_level = int(sorted_lens[l_level])\n",
    "        \n",
    "        stats['L' + str(level)] = l_level\n",
    "\n",
    "        # Store the statistic in a dictionary, mapped to its new name key\n",
    "        stats['N' + str(level)] = n_level\n",
    "        \n",
    "    return stats\n",
    "\n",
    "\n",
    "#---- assemblyDictionary\n",
    "def assemblyDictionary(infilename):\n",
    "    \n",
    "    # Return two np arrays of lengths\n",
    "    contig_lens, scaffold_lens, gc_cont = read_genome(infilename)\n",
    "    \n",
    "    # Return contig stats from contig lengths\n",
    "    contig_stats = calculate_stats(contig_lens, gc_cont)\n",
    "    \n",
    "    # Return scaffold stats from scaffold lengths\n",
    "    scaffold_stats = calculate_stats(scaffold_lens, gc_cont)\n",
    "    \n",
    "    # A dictionary of outputs is easily queried\n",
    "    stat_output = {'Contig Stats': contig_stats,\n",
    "                   'Scaffold Stats': scaffold_stats}\n",
    "    \n",
    "    return stat_output\n",
    "```"
   ],
   "id": "76d2e2b3-391d-40e5-a639-3bab42bd5045"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"03_Load/scripts/strint.py\")"
   ],
   "id": "c620b0cf-5189-4846-b133-8c89f84e283d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsDict = assemblyDictionary(\"02_Transform/data/magicicada.fasta\")"
   ],
   "id": "7476fcb8-ee8e-44d9-8ac3-e28238697a1a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Load\n",
    "\n",
    "This is the data loading phase. Following completion of this stage, querying the data should be more intuitive than before.\n",
    "\n",
    "### 3.1 Assign Variables with `strint.py`\n",
    "\n",
    "``` python\n",
    "\"\"\" \n",
    "Formats the string representation of an \n",
    "integer value as a comma separated string \n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def strint(data, category, label):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # Existing DataFrame handling code\n",
    "        stat = data.loc[(category, label), \"Value\"]\n",
    "        \n",
    "        # Set boolean match value\n",
    "        isFloat = re.search(r\"\\.\", str(stat))\n",
    "        \n",
    "        # Convert to float if there is a decimal\n",
    "        if isFloat:\n",
    "            stat = pd.to_numeric(stat, downcast=\"float\")\n",
    "        else:\n",
    "            # Else convert to an integer \n",
    "            stat = pd.to_numeric(stat, downcast=\"integer\")\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        # New dictionary handling code\n",
    "        #stat = data.get(f\"{category} {label}\")\n",
    "        stat = data[category][label]\n",
    "        \n",
    "        if stat is None:\n",
    "            raise KeyError(f\"Key '{category} {label}' not found in the dictionary\")\n",
    "        \n",
    "        # No need to convert to numeric as values are already integers or floats\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(\"Input must be a pandas DataFrame or a dictionary\")\n",
    "\n",
    "    # Add a thousands separator and convert back to a string\n",
    "    stat = f'{stat:,}'\n",
    "    \n",
    "    return stat\n",
    "```\n",
    "\n",
    "### 3.2 DataFrame Input"
   ],
   "id": "4a1aa991-bbbe-4f1d-b0e9-aac142ea8eaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Contigs\n",
    "ctig_len = strint(df, \"Contigs\", \"total_bps\")\n",
    "ctig_count = strint(df, \"Contigs\", \"sequence_count\")\n",
    "ctig_n50 = strint(df, \"Contigs\", \"N50\")\n",
    "ctig_max = strint(df, \"Contigs\", \"longest\")\n",
    "\n",
    "#---- Scaffolds\n",
    "sfld_len = strint(df, \"Scaffolds\", \"total_bps\")\n",
    "sfld_count = strint(df, \"Scaffolds\", \"sequence_count\")\n",
    "sfld_n50 = strint(df, \"Scaffolds\", \"N50\")\n",
    "sfld_max = strint(df, \"Scaffolds\", \"longest\")"
   ],
   "id": "c335a5e1-2249-40a6-a7ac-fa6ca0de253f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Multi-index dataframe query syntax**\n",
    ">\n",
    "> ``` python\n",
    "> \"\"\"\n",
    ">\n",
    "> strint(dataframe, category, label)\n",
    ">\n",
    "> category options\n",
    "> ----------------\n",
    ">   Contigs\n",
    ">   Scaffolds\n",
    ">\n",
    ">\n",
    "> label options\n",
    "> ----------------\n",
    ">   L10\n",
    ">   L20\n",
    ">   L30\n",
    ">   L40\n",
    ">   L50\n",
    ">   N10\n",
    ">   N20\n",
    ">   N30\n",
    ">   N40\n",
    ">   N50\n",
    ">   gc_content\n",
    ">   longest\n",
    ">   mean\n",
    ">   median\n",
    ">   sequence_count\n",
    ">   shortest\n",
    ">   total_bps\n",
    ">   \n",
    "> \"\"\"\n",
    ">\n",
    "> ctig_len = strint(df, \"Contigs\", \"N50\")\n",
    ">\n",
    "> # -->> Looking inside the strint(dataframe, category, label) function ---->>\n",
    ">\n",
    "> # Which then finds the desired value or 'Value'\n",
    "> stat = df.loc[(category, label), \"Value\"]\n",
    "> ```\n",
    "\n",
    "### 3.3 Dictionary Input"
   ],
   "id": "4384a89d-32c3-4193-afab-c3e3b2f279bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Contigs\n",
    "ctig_len = strint(statsDict, \"Contig Stats\", \"total_bps\")\n",
    "ctig_count = strint(statsDict, \"Contig Stats\", \"sequence_count\")\n",
    "ctig_n50 = strint(statsDict, \"Contig Stats\", \"N50\")\n",
    "ctig_max = strint(statsDict, \"Contig Stats\", \"longest\")\n",
    "\n",
    "#---- Scaffolds\n",
    "sfld_len = strint(statsDict, \"Scaffold Stats\", \"total_bps\")\n",
    "sfld_count = strint(statsDict, \"Scaffold Stats\", \"sequence_count\")\n",
    "sfld_n50 = strint(statsDict, \"Scaffold Stats\", \"N50\")\n",
    "sfld_max = strint(statsDict, \"Scaffold Stats\", \"longest\")"
   ],
   "id": "2289e03a-f9e4-47fc-af24-77c77b6423b2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dictionary query syntax**\n",
    ">\n",
    "> ``` python\n",
    "> \"\"\"\n",
    ">\n",
    "> strint(dataframe, category, label)\n",
    ">\n",
    "> category options\n",
    "> ----------------\n",
    ">   Contig Stats\n",
    ">   Scaffold Stats\n",
    ">\n",
    ">\n",
    "> label options\n",
    "> ----------------\n",
    ">   L10\n",
    ">   L20\n",
    ">   L30\n",
    ">   L40\n",
    ">   L50\n",
    ">   N10\n",
    ">   N20\n",
    ">   N30\n",
    ">   N40\n",
    ">   N50\n",
    ">   gc_content\n",
    ">   longest\n",
    ">   mean\n",
    ">   median\n",
    ">   sequence_count\n",
    ">   shortest\n",
    ">   total_bps\n",
    ">   \n",
    "> \"\"\"\n",
    ">\n",
    "> # A look inside assemblyDictionary.py where stat_output is returned\n",
    "> stat_output = {'Contig Stats': contig_stats,\n",
    ">                'Scaffold Stats': scaffold_stats}\n",
    ">\n",
    "> ctig_len = statsDict[\"Contig Stats\"][\"total_bps\"]\n",
    "> ```\n",
    "\n",
    "## 4 Present\n",
    "\n",
    "### 4.1 The Pandas Table\n",
    "\n",
    "This is a slightly formatted view of the Pandas table designed to be more easily queried to return the desired statistic. If, however, you’d like to treat the Styler object as the unchanged, dataframe object, use the `forma_df.data` syntax.\n",
    "\n",
    "[:The original dataframe output:](#NutFrame)"
   ],
   "id": "3ba962be-396a-4c53-bb37-9ced9e57971a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n"
      ]
     }
    }
   ],
   "source": [
    "# Display with Style \n",
    "forma_df = df.loc[:].style.pipe(formaFrame)\n",
    "\n",
    "forma_df"
   ],
   "id": "27a9f198-4272-4b23-ab0e-03ec6cdfafd3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 DNA Zoo’s Table, Reproduced"
   ],
   "id": "e035d963-3232-4671-a2de-291ee6109f3d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(reticulate)"
   ],
   "id": "2411ab9f-e6f0-465f-9072-620473f56885"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the library makes it simpler for inserting the values into the table below. For example, I would have had to type 4,902,968.0, but now, I only need to type 4,902,968.0 into the individual cells. I needed to convert the Python into R objects, as the knitr engine used in rendering this document does not seem to display output from execution of inline Python code directly.\n",
    "\n",
    "|  |  |  |  |\n",
    "|------------------|------------------|------------------|-------------------|\n",
    "| **Contig length (bp)** | **Number of contigs** | **Contig N50 (bp)** | **Longest contig (bp)** |\n",
    "| 6,520,445,364.0 | 4,200.0 | 4,902,968.0 | 43,529,772.0 |\n",
    "| **Scaffold length (bp)** | **Number of scaffolds** | **Scaffold N50 (bp)** | **Longest scaffold (bp)** |\n",
    "| 6,521,530,364.0 | 2,030.0 | 518,932,092.0 | 1,438,277,616.0 |\n",
    "\n",
    "## :x NutFrame"
   ],
   "id": "073406b0-c53d-4ae1-a6e2-63e2422aa20b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                 Value\n",
      "Category  Label                       \n",
      "Contigs   L10             4.100000e+01\n",
      "          L20             9.900000e+01\n",
      "          L30             1.740000e+02\n",
      "          L40             2.670000e+02\n",
      "          L50             3.850000e+02\n",
      "          N10             1.264377e+07\n",
      "          N20             9.681846e+06\n",
      "          N30             7.895799e+06\n",
      "          N40             6.288966e+06\n",
      "          N50             4.902968e+06\n",
      "          gc_content      3.524810e+01\n",
      "          longest         4.352977e+07\n",
      "          mean            1.552487e+06\n",
      "          median          3.319350e+05\n",
      "          sequence_count  4.200000e+03\n",
      "          shortest        1.000000e+03\n",
      "          total_bps       6.520445e+09\n",
      "Scaffolds L10             0.000000e+00\n",
      "          L20             0.000000e+00\n",
      "          L30             1.000000e+00\n",
      "          L40             2.000000e+00\n",
      "          L50             3.000000e+00\n",
      "          N10             1.438278e+09\n",
      "          N20             1.438278e+09\n",
      "          N30             9.154918e+08\n",
      "          N40             6.075082e+08\n",
      "          N50             5.189321e+08\n",
      "          gc_content      3.524810e+01\n",
      "          longest         1.438278e+09\n",
      "          mean            3.212577e+06\n",
      "          median          6.236250e+04\n",
      "          sequence_count  2.030000e+03\n",
      "          shortest        1.000000e+03\n",
      "          total_bps       6.521530e+09"
     ]
    }
   ],
   "source": [
    "# The un-styled dataframe output\n",
    "forma_df.data"
   ],
   "id": "748f4d74-2844-4bf7-9bd5-0858af7a6505"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DNA Zoo*. <https://www.dnazoo.org>\n",
    "\n",
    "*Dnazoo.s3*. <https://dnazoo.s3.wasabisys.com/index.html?prefix=Magicicada_septendecula/>\n",
    "\n",
    "*Little 17-year cicada*. (2023). <https://www.dnazoo.org/assemblies/magicicada_septendecula>\n",
    "\n",
    "Trizna, M. (2020). *Assembly\\_stats 0.1.4*. Zenodo. <https://doi.org/10.5281/ZENODO.3968774>"
   ],
   "id": "e9cabae9-4958-4a97-98db-3d73f988b40e"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
