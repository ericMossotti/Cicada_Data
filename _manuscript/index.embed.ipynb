{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cicada Data\n",
    "\n",
    "Data analysis reproduction concerning the cicada’s genome.\n",
    "\n",
    "Eric Mossotti  \n",
    "Jul 15, 2024\n",
    "\n",
    "To reproduce DNA Zoo’s summary table on the 17-year cicada.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Problem\n",
    "\n",
    "The steps involved in reproducing data can be unclear.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "To elaborate on the objective stated at the top of this document, I seek to supplement DNA Zoo’s report with an accessible data analysis (DA) pipeline. To accomplish this, I independently reproduce the original article’s table while documenting every data processing step. Although there’s nothing wrong with the original works, things can always be taken further. \\[@dnazoo\\], \\[@magicica\\]\n",
    "\n",
    "### Stakeholders\n",
    "\n",
    "This might be of interest to the original authors of the article. More generally, the spirit of this work could transfer to other domains of data intensive research and analytics.\n",
    "\n",
    "### Source\n",
    "\n",
    "All data used within this report was freely available from a public database hosted by DNA Zoo. \\[@dnazoo.s\\]\n",
    "\n",
    "## Pipeline"
   ],
   "id": "487acefd-913e-46f9-bc80-c5d83ae725e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "layout-align": "default"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {
      "text/html": [
       "<pre class=\"mermaid mermaid-js\">\n",
       "flowchart TB\n",
       "    A((1)):::circle --&gt; B((2)):::circle\n",
       "    B --&gt; C((3)):::circle\n",
       "    C --&gt; D((4)):::circle\n",
       "\n",
       "    subgraph Extract [&quot;1. Extract&quot;]\n",
       "        direction LR\n",
       "        A1[&quot;directorize.py&quot;] --&gt; A2[&quot;importer.py&quot;]\n",
       "    end\n",
       "    subgraph Transform [&quot;2. Transform&quot;]\n",
       "      direction TB\n",
       "      B1{&quot;decompress.py&quot;} -.-&gt;|.fasta| B2[&quot;assembly_stats&quot;] -.-&gt;|summary_stats.txt| B3[&quot;assemblyFramer.py&quot;]\n",
       "      B1 -.-&gt;|.fasta| B4[&quot;assemblyDictionary.py&quot;]\n",
       "    end\n",
       "    subgraph Load [&quot;3. Load&quot;]\n",
       "        direction TB\n",
       "        C1{&quot;strint.py&quot;}\n",
       "    end\n",
       "    subgraph Present [&quot;4. Present&quot;]\n",
       "        direction TB\n",
       "        D1[&quot;DNA Zoo&#39;s Table, Reproduced&quot;]\n",
       "    end\n",
       "    \n",
       "    A ~~~ Extract\n",
       "    B ~~~ Transform\n",
       "    C ~~~ Load\n",
       "    D ~~~ Present\n",
       "    \n",
       "    A2 -.-&gt;|fasta.gz| B1\n",
       "    B3 -.-&gt;|dataframe| C1\n",
       "    B4 -.-&gt;|dict| C1\n",
       "    C1 -.-&gt;|strings| Present\n",
       "    C1 -.-&gt;|strings| Present\n",
       "    \n",
       "</pre>"
      ]
     }
    }
   ],
   "source": [
    "flowchart TB\n",
    "    A((1)):::circle --> B((2)):::circle\n",
    "    B --> C((3)):::circle\n",
    "    C --> D((4)):::circle\n",
    "\n",
    "    subgraph Extract [\"1. Extract\"]\n",
    "        direction LR\n",
    "        A1[\"directorize.py\"] --> A2[\"importer.py\"]\n",
    "    end\n",
    "    subgraph Transform [\"2. Transform\"]\n",
    "      direction TB\n",
    "      B1{\"decompress.py\"} -.->|.fasta| B2[\"assembly_stats\"] -.->|summary_stats.txt| B3[\"assemblyFramer.py\"]\n",
    "      B1 -.->|.fasta| B4[\"assemblyDictionary.py\"]\n",
    "    end\n",
    "    subgraph Load [\"3. Load\"]\n",
    "        direction TB\n",
    "        C1{\"strint.py\"}\n",
    "    end\n",
    "    subgraph Present [\"4. Present\"]\n",
    "        direction TB\n",
    "        D1[\"DNA Zoo's Table, Reproduced\"]\n",
    "    end\n",
    "    \n",
    "    A ~~~ Extract\n",
    "    B ~~~ Transform\n",
    "    C ~~~ Load\n",
    "    D ~~~ Present\n",
    "    \n",
    "    A2 -.->|fasta.gz| B1\n",
    "    B3 -.->|dataframe| C1\n",
    "    B4 -.->|dict| C1\n",
    "    C1 -.->|strings| Present\n",
    "    C1 -.->|strings| Present"
   ],
   "id": "ca19b1c9-e3fe-4aeb-a975-3670d4551fbb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract\n",
    "\n",
    "This would be the data extraction phase of the DA pipeline.\n",
    "\n",
    "### Create Project Directory"
   ],
   "id": "5c823413-a91b-4929-b288-e715e7b8d97e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automates creation of the DA pipeline directory needed for this project to avoid confusion for those replicating this project, if desired.\n",
    "\n",
    "import os\n",
    "\n",
    "def directorize(base_path, structure):\n",
    "    # Nested directories\n",
    "    for dir_name, subdirs in structure.items():\n",
    "        dir_path = os.path.join(base_path, dir_name)\n",
    "        os.makedirs(dir_path, exist_ok = True)\n",
    "\n",
    "        for subdir in subdirs:\n",
    "            subdir_path = os.path.join(dir_path, subdir)\n",
    "            os.makedirs(subdir_path, exist_ok = True)"
   ],
   "id": "baf0eddf-245c-4bfe-b225-6e1047e63d2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory structure\n",
    "structure = {\n",
    "    \"01_Extract/\": [\"data/\", \"scripts/\"],\n",
    "    \"02_Transform/\": [\"data/\", \"scripts/\"],\n",
    "    \"03_Load/\": [\"data/\", \"scripts/\"],\n",
    "    \"04_Present/\": [\"data/\", \"scripts/\"]\n",
    "}\n",
    "\n",
    "# Create the analysis folder structure in a preferred base directory\n",
    "# \"\" = project's working directory\n",
    "directorize(\"\", structure)"
   ],
   "id": "0ae959ab-5b91-4c95-a332-a5ec6b1876a9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download to Local Machine"
   ],
   "id": "c38481e2-5ed7-4e2f-a01c-0f7217e32cbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Import data from the web with wget\n",
    "import os\n",
    "import sys\n",
    "import wget\n",
    "\n",
    "def importer (fileMap):\n",
    "    # Download from URL to path and notify when complete\n",
    "    for url, file_path in fileMap.items():\n",
    "        # Checking file existence\n",
    "        if not os.path.exists(file_path):\n",
    "            wget.download(url, file_path)\n",
    "            print(f\"{file_path} written\")\n",
    "        else:\n",
    "            print(f\"{file_path} already exists.\")"
   ],
   "id": "adb3f932-b423-4e15-b71e-cbd2c4e3e0be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "01_Extract/data/magicicada.fasta.gz written\n",
      "02_Transform/scripts/decompress.py written\n",
      "02_Transform/scripts/assemblyDictionary.py written\n",
      "02_Transform/scripts/assemblyFramer.py written\n",
      "03_Load/scripts/strint.py written\n",
      "04_Present/scripts/formaFrame.py written\n",
      "01_Extract/data/.gitignore written\n",
      "02_Transform/data/.gitignore written"
     ]
    }
   ],
   "source": [
    "# Set the url\n",
    "fasta_URL = \"https://dnazoo.s3.wasabisys.com/Magicicada_septendecula/magicicada_hifiasm.asm.bp.p_ctg_HiC.fasta.gz\"\n",
    "# Set the desired local file path\n",
    "fasta_PATH = \"01_Extract/data/magicicada.fasta.gz\"\n",
    "\n",
    "decompress_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/02_Transform/scripts/decompress.py\"\n",
    "decompress_PATH = \"02_Transform/scripts/decompress.py\"\n",
    "\n",
    "asmblydict_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/02_Transform/scripts/assemblyDictionary.py\"\n",
    "asmblydict_PATH = \"02_Transform/scripts/assemblyDictionary.py\"\n",
    "\n",
    "asmblyframe_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/02_Transform/scripts/assemblyFramer.py\"\n",
    "asmblyframe_PATH = \"02_Transform/scripts/assemblyFramer.py\"\n",
    "\n",
    "strint_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/03_Load/scripts/strint.py\"\n",
    "strint_PATH = \"03_Load/scripts/strint.py\"\n",
    "\n",
    "formaframe_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/04_Present/scripts/formaFrame.py\"\n",
    "formaframe_PATH = \"04_Present/scripts/formaFrame.py\"\n",
    "\n",
    "extractIgnore_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/01_Extract/data/.gitignore\"\n",
    "extractIgnore_PATH = \"01_Extract/data/.gitignore\"\n",
    "\n",
    "transformIgnore_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/02_Transform/data/.gitignore\"\n",
    "transformIgnore_PATH = \"02_Transform/data/.gitignore\"\n",
    "\n",
    "\n",
    "# Map the url to the file path\n",
    "fileMap = {\n",
    "  fasta_URL: fasta_PATH,\n",
    "  decompress_URL: decompress_PATH,\n",
    "  asmblydict_URL: asmblydict_PATH,\n",
    "  asmblyframe_URL: asmblyframe_PATH,\n",
    "  strint_URL: strint_PATH,\n",
    "  formaframe_URL: formaframe_PATH,\n",
    "  extractIgnore_URL: extractIgnore_PATH,\n",
    "  transformIgnore_URL: transformIgnore_PATH\n",
    "  }\n",
    "\n",
    "importer(fileMap)"
   ],
   "id": "434eb952-f21f-4113-9d94-30a910368e63"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The specific link used to download all data from**\n",
    ">\n",
    "> <https://dnazoo.s3.wasabisys.com/Magicicada_septendecula/magicicada_hifiasm.asm.bp.p_ctg_HiC.fasta.gz>\n",
    "\n",
    "## Transform\n",
    "\n",
    "The data transformation phase of the pipeline.\n",
    "\n",
    "### Decompress .GZ"
   ],
   "id": "41c33116-072e-44af-b639-a7c6d8f6275f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"02_Transform/scripts/decompress.py\")"
   ],
   "id": "bbbd4f4a-a276-430b-a40b-daf5b9318497"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Decompress the gz file with gzip\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def decompress(gzFasta, fasta):\n",
    "    \n",
    "    # If not decompressed, then decompress and redirect to a new file path\n",
    "    if not os.path.exists(fasta):\n",
    "        # File doesn't exist, then decompress\n",
    "        with gzip.open(gzFasta, 'rb') as f_in:\n",
    "            with open(fasta, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"{fasta} has been decompressed and written.\")\n",
    "    else:\n",
    "        print(f\"The file {fasta} already exists. Skipping unzip.\")"
   ],
   "id": "7296281e-b8ed-4e2d-807c-e1d26ef47ab9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "02_Transform/data/magicicada.fasta has been decompressed and written."
     ]
    }
   ],
   "source": [
    "# Set the compressed fasta.gz file variable\n",
    "gzFasta = \"01_Extract/data/magicicada.fasta.gz\"\n",
    "\n",
    "# Set the decompressed fasta file variable\n",
    "fasta = \"02_Transform/data/magicicada.fasta\"\n",
    "\n",
    "# Pass file paths to the function\n",
    "decompress(gzFasta, fasta)"
   ],
   "id": "05108af0-92d3-4495-896f-b675741369f8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FASTA to Text, to DataFrame\n",
    "\n",
    "Sourcing the dataframe formatting script here, as there is some issue with knitr engine and Python with these parts. Seems like some kind of conflict with switching between dataframe and dictionary data types."
   ],
   "id": "9f7417ee-3d0f-47bb-81f6-52db01cd7c3d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"04_Present/scripts/formaFrame.py\")"
   ],
   "id": "42e4fc53-fbbe-4bc6-bc22-f823047e26fd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chunk should be ran locally instead of with `quarto render`. When working with the source file, change the code-chunk language specifier from `{.bash}` back to `{bash}`. You might have to add the `{bash}` tag entirely back to the div. Not sure how else to go about accomplishing this within my current Quarto project setup. \\[@trizna2020\\]\n",
    "\n",
    "``` bash\n",
    "# BASH SCRIPT\n",
    "\n",
    "# The uncompressed fasta file variable\n",
    "fasta=02_Transform/data/magicicada.fasta\n",
    "\n",
    "# The text file path variable generated by the script\n",
    "summary_stats=02_Transform/data/summary_stats.txt\n",
    "\n",
    "assembly_stats $fasta > $summary_stats\n",
    "```\n",
    "\n",
    "I need to do this for the rendering part to display the code-cell outputs how I wanted them to. However, you can use the bash code chunk further below and comment this code out if you want. This makes it so you can deactivate the bash code chunk for render but still have everything else work without a hitch if you go simply from empty directory to render with this document."
   ],
   "id": "9e722627-5ff6-4358-a6d6-a70aaf7d8c78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "02_Transform/data/summary_stats.txt written"
     ]
    }
   ],
   "source": [
    "summaryStats_URL = \"https://raw.githubusercontent.com/ericMossotti/Cicada_Data/main/02_Transform/data/summary_stats.txt\"\n",
    "\n",
    "summaryStats_PATH = \"02_Transform/data/summary_stats.txt\"\n",
    "\n",
    "map_summary = {summaryStats_URL: summaryStats_PATH}\n",
    "\n",
    "importer(map_summary)"
   ],
   "id": "f703e302-b6f2-4622-b24d-d8f5742edf66"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, transform the text file into a Python dataframe. I am opting not to blanket change data-types as output format could vary by user preference."
   ],
   "id": "adf4e503-7ebf-4b63-8a13-59c88a9e279e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import external python script to local library environment\n",
    "reticulate::source_python(\"02_Transform/scripts/assemblyFramer.py\")"
   ],
   "id": "d20138cb-9b35-4b2b-b60b-18b3c8f60b80"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\"\"\" Utilizes Python string methods and multi-indexing \n",
    "to process assembly_stats' output text file \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def assemblyFramer(statsPath = None):\n",
    "    \n",
    "    #---- Read Text File\n",
    "    with open(statsPath, 'r') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "    #---- Regex Matching\n",
    "    pairs = re.findall(r\"\\\"\\w+\\\"\\:\\s\\d*\\.?\\d*\", content)\n",
    "    \n",
    "    #---- Clean Strings\n",
    "    cleaned_list = [pair.replace('\"', '').replace(':', '').strip() for pair in pairs]\n",
    "    \n",
    "    #---- Split Strings\n",
    "    labeled_list = [item.split() for item in cleaned_list]\n",
    "    \n",
    "    #---- Create DataFrame\n",
    "    df = pd.DataFrame(labeled_list, columns = ['Label', 'Value'])\n",
    "    \n",
    "    #---- Add Category Column\n",
    "    df['Category'] = ['Contigs'] * 17 + ['Scaffolds'] * 17\n",
    "    \n",
    "    #---- Create Arrays\n",
    "    category_array = pd.Series.to_list(df['Category'])\n",
    "    label_array = pd.Series.to_list(df['Label'])\n",
    "    value_array = pd.Series.to_list(df['Value'])\n",
    "    \n",
    "    #---- Combine Arrays to List\n",
    "    arrayList = [category_array, label_array]\n",
    "    \n",
    "    #---- Define Multi-Level Indices\n",
    "    indices = pd.MultiIndex.from_arrays(arrays = arrayList, names = ('Category', 'Label'))\n",
    "    \n",
    "    #---- Index a DataFrame \n",
    "    df_indexed = pd.DataFrame(data = value_array, index = indices)\n",
    "    \n",
    "    #---- Rename Non-Indexed Column\n",
    "    df_indexed = df_indexed.rename(columns = {0:\"Value\"})\n",
    "    \n",
    "    return df_indexed\n",
    "```"
   ],
   "id": "c2ee5eb2-9335-4fa1-b987-22633e1bd757"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the local text file path\n",
    "statsPath = \"02_Transform/data/summary_stats.txt\"\n",
    "# Run to yield a multi-indexed dataframe\n",
    "df = assemblyFramer(statsPath)"
   ],
   "id": "eda658e8-1b78-4fe7-ab46-557cb6be4ef7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FASTA to Dictionary"
   ],
   "id": "928d9ebe-4683-4390-8c37-a4baf56acdb8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"02_Transform/scripts/assemblyDictionary.py\")"
   ],
   "id": "a6a35763-1282-4ea2-88c8-a3bcf76fca6d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\"\"\"\n",
    "Parsing genomic data in a memory-efficent way by not loading the entire\n",
    "file into memory at once. The file is processed one line at a time, grouping\n",
    "related lines together. \n",
    "\n",
    "Statistics such as N50 are crucial in assessing the contiguity \n",
    "of a genome assembly, with higher N50 values generally indicating \n",
    "a more contiguous assembly.\n",
    "\n",
    "The distinction between contigs and scaffolds is important in genome assembly, \n",
    "as it provides information about the continuity and completeness of \n",
    "the assembly.\n",
    "\n",
    "\n",
    "#----\n",
    "read_genome()\n",
    "____________\n",
    "\n",
    "Differentiates between scaffolds (which may contain gaps) and contigs \n",
    "(continuous sequences). \n",
    "  \n",
    "Calculate the GC content, which is an important genomic characteristic.\n",
    "Prepare lists of contig and scaffold lengths.\n",
    "\n",
    "\n",
    "#----\n",
    "fasta_iter()\n",
    "____________\n",
    "\n",
    "Groups the .fasta file data into alternating groups of headers and sequences.\n",
    "It is a generator function that will pause until the next item is requested \n",
    "after yielding a tuple.\n",
    "\n",
    "The iterator groups two aspects:\n",
    "  a. Single header lines starting with '>'\n",
    "  b. Subsequent lines until the next '>'\n",
    "\n",
    "\n",
    "#----\n",
    "calculate_stats()\n",
    "_________________\n",
    "\n",
    "Where the stats dictionary values are calculated and keys assigned.\n",
    "\n",
    "\n",
    "#----\n",
    "def assemblyDictionary()\n",
    "________________________\n",
    "\n",
    "Maps the previously created dictionaries of contig and scaffold stats to the \n",
    "category they belong, thereby distinguishing the contigs and scaffold stats. \n",
    "Returning the desired values is quite intuitive as a result.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "\n",
    "#---- fasta_iter()\n",
    "def fasta_iter(fasta_file):\n",
    "    \n",
    "    fh = open(fasta_file)\n",
    "    \n",
    "    # Only need the second part, or code sequences part, of the grouped by items\n",
    "    fa_iter = (x[1] for x in groupby(fh, lambda line: line[0] == \">\"))\n",
    "    \n",
    "    for header in fa_iter:\n",
    "        \n",
    "        # Get first line of group; drop the \">\" and starting/trailing whitespace\n",
    "        header = next(header)[1:].strip()\n",
    "        \n",
    "        # Join all sequence lines to one string; conv to uppercase; remv whitespace\n",
    "        seq = \"\".join(s.upper().strip() for s in next(fa_iter))\n",
    "        \n",
    "        yield header, seq\n",
    "\n",
    "\n",
    "#---- read_genome\n",
    "def read_genome(fasta_file):\n",
    "    \n",
    "    gc = 0\n",
    "    total_len = 0\n",
    "    \n",
    "    contig_lens = []\n",
    "    scaffold_lens = []\n",
    "    \n",
    "    # Ignore header information (the '_' part) and process sequence data\n",
    "    for _, seq in fasta_iter(fasta_file):\n",
    "        \n",
    "        # Add sequence (scaffold) length\n",
    "        scaffold_lens.append(len(seq))\n",
    "        # NN reprs gaps in scaffold, which are contigs\n",
    "        if \"NN\" in seq:\n",
    "            # Add split sequences to contig list if gap\n",
    "            contig_list = seq.split(\"NN\")\n",
    "            \n",
    "        else:\n",
    "            # Add sequence to contig list\n",
    "            contig_list = [seq]\n",
    "            \n",
    "        for contig in contig_list:\n",
    "            # An initial check for 0-length contigs\n",
    "            if len(contig):\n",
    "              gc += contig.count('G') + contig.count('C')\n",
    "              # Update the total length\n",
    "              total_len += len(contig)\n",
    "              # Add  length to list of contig lengths\n",
    "              contig_lens.append(len(contig))\n",
    "    # GC content as the percentage of total genome\n",
    "    gc_cont = (gc / total_len) * 100\n",
    "\n",
    "    return contig_lens, scaffold_lens, gc_cont\n",
    "\n",
    "\n",
    "#---- calculate_stats()\n",
    "def calculate_stats(seq_lens, gc_cont):\n",
    "    \n",
    "    # Empty dictionary\n",
    "    stats = {}\n",
    "    # The set of sequence lengths are converted to a NumPy array\n",
    "    seq_array = np.array(seq_lens)\n",
    "    \n",
    "    # Count the individual sequences\n",
    "    stats['sequence_count'] = seq_array.size\n",
    "    \n",
    "    # GC proportion\n",
    "    stats['gc_content'] = gc_cont\n",
    "\n",
    "    # Sort lengths by descending order\n",
    "    sorted_lens = seq_array[np.argsort(-seq_array)]\n",
    "    \n",
    "    # The first length is the longest due to sorting\n",
    "    stats['longest'] = int(sorted_lens[0])\n",
    "    \n",
    "    # Likewise, shortest length is at the end\n",
    "    stats['shortest'] = int(sorted_lens[-1])\n",
    "    \n",
    "    stats['median'] = np.median(sorted_lens)\n",
    "    \n",
    "    stats['mean'] = np.mean(sorted_lens)\n",
    "    \n",
    "    # Sums the total length of all sequences\n",
    "    stats['total_bps'] = int(np.sum(sorted_lens))\n",
    "    \n",
    "    # An array of cumulative sums to calculate N50 efficiently\n",
    "    csum = np.cumsum(sorted_lens)\n",
    "    \n",
    "    for level in [10, 20, 30, 40, 50]:\n",
    "        \n",
    "        # Calculate target base pair count for the level\n",
    "        nx = int(stats['total_bps'] * (level / 100))\n",
    "        \n",
    "        # Find smallest bp value in array, >= to the target %\n",
    "        csumn = min(csum[csum >= nx])\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        --- The original code in the next line:\n",
    "  \n",
    "          l_level = int(np.where(csum == csumn)[0])\n",
    "          \n",
    "        --- Has been changed to:\n",
    "            \n",
    "          l_level = int(np.where(csum == csumn)[0][0])\n",
    "\n",
    "        This finds the index where the cumulative sum equals csumn, which \n",
    "        represents the number of sequences needed to reach the target \n",
    "        percentage of base pairs. \n",
    "        \n",
    "        I've added an extra [0] to fix the NumPy deprecation warning. This \n",
    "        ensures return of a scalar value from the array, as the extra '[0]' \n",
    "        ensures the first element of the first array is being accessed.\n",
    "        \n",
    "        The console warning (in Rstudio) that's now been resolved: \n",
    "        \n",
    "          'Conversion of an array with ndim > 0 to a scalar is deprecated, and \n",
    "          will error in future. Ensure you extract a single element from your \n",
    "          array before performing this operation. (Deprecated NumPy 1.25.)'\n",
    "          \n",
    "        \"\"\"\n",
    "        \n",
    "        # Determine the index where the sequences required to reach target % of bps is met\n",
    "        l_level = int(np.where(csum == csumn)[0][0])\n",
    "        \n",
    "        # Get bp length of seq at index, l_level, for the N-statistic value\n",
    "        n_level = int(sorted_lens[l_level])\n",
    "        \n",
    "        stats['L' + str(level)] = l_level\n",
    "\n",
    "        # Store the statistic in a dictionary, mapped to its new name key\n",
    "        stats['N' + str(level)] = n_level\n",
    "        \n",
    "    return stats\n",
    "\n",
    "\n",
    "#---- assemblyDictionary\n",
    "def assemblyDictionary(infilename):\n",
    "    \n",
    "    # Return two np arrays of lengths\n",
    "    contig_lens, scaffold_lens, gc_cont = read_genome(infilename)\n",
    "    \n",
    "    # Return contig stats from contig lengths\n",
    "    contig_stats = calculate_stats(contig_lens, gc_cont)\n",
    "    \n",
    "    # Return scaffold stats from scaffold lengths\n",
    "    scaffold_stats = calculate_stats(scaffold_lens, gc_cont)\n",
    "    \n",
    "    # A dictionary of outputs is easily queried\n",
    "    stat_output = {'Contig Stats': contig_stats,\n",
    "                   'Scaffold Stats': scaffold_stats}\n",
    "    \n",
    "    return stat_output\n",
    "```"
   ],
   "id": "edef0834-f37e-4e84-8747-cc2f68288c09"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reticulate::source_python(\"03_Load/scripts/strint.py\")"
   ],
   "id": "297a1042-b604-4f62-91d2-d4542e387177"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsDict = assemblyDictionary(\"02_Transform/data/magicicada.fasta\")"
   ],
   "id": "6238a5d1-f885-4bd3-80ee-6033d64511cd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "This is the data loading phase. Following completion of this stage, querying the data should be more intuitive than before.\n",
    "\n",
    "### Assign Variables with `strint.py`\n",
    "\n",
    "``` python\n",
    "\"\"\" \n",
    "Formats the string representation of an \n",
    "integer value as a comma separated string \n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def strint(data, category, label):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # Existing DataFrame handling code\n",
    "        stat = data.loc[(category, label), \"Value\"]\n",
    "        \n",
    "        # Set boolean match value\n",
    "        isFloat = re.search(r\"\\.\", str(stat))\n",
    "        \n",
    "        # Convert to float if there is a decimal\n",
    "        if isFloat:\n",
    "            stat = pd.to_numeric(stat, downcast=\"float\")\n",
    "        else:\n",
    "            # Else convert to an integer \n",
    "            stat = pd.to_numeric(stat, downcast=\"integer\")\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        # New dictionary handling code\n",
    "        #stat = data.get(f\"{category} {label}\")\n",
    "        stat = data[category][label]\n",
    "        \n",
    "        if stat is None:\n",
    "            raise KeyError(f\"Key '{category} {label}' not found in the dictionary\")\n",
    "        \n",
    "        # No need to convert to numeric as values are already integers or floats\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(\"Input must be a pandas DataFrame or a dictionary\")\n",
    "\n",
    "    # Add a thousands separator and convert back to a string\n",
    "    stat = f'{stat:,}'\n",
    "    \n",
    "    return stat\n",
    "```\n",
    "\n",
    "### DataFrame Input"
   ],
   "id": "d60ec16f-0a16-4fed-90ce-127ee1105479"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Contigs\n",
    "ctig_len = strint(df, \"Contigs\", \"total_bps\")\n",
    "ctig_count = strint(df, \"Contigs\", \"sequence_count\")\n",
    "ctig_n50 = strint(df, \"Contigs\", \"N50\")\n",
    "ctig_max = strint(df, \"Contigs\", \"longest\")\n",
    "\n",
    "#---- Scaffolds\n",
    "sfld_len = strint(df, \"Scaffolds\", \"total_bps\")\n",
    "sfld_count = strint(df, \"Scaffolds\", \"sequence_count\")\n",
    "sfld_n50 = strint(df, \"Scaffolds\", \"N50\")\n",
    "sfld_max = strint(df, \"Scaffolds\", \"longest\")"
   ],
   "id": "25e85968-0162-4950-a31b-90f5ccaab786"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Multi-index dataframe query syntax**\n",
    ">\n",
    "> ``` python\n",
    "> \"\"\"\n",
    ">\n",
    "> strint(dataframe, category, label)\n",
    ">\n",
    "> category options\n",
    "> ----------------\n",
    ">   Contigs\n",
    ">   Scaffolds\n",
    ">\n",
    ">\n",
    "> label options\n",
    "> ----------------\n",
    ">   L10\n",
    ">   L20\n",
    ">   L30\n",
    ">   L40\n",
    ">   L50\n",
    ">   N10\n",
    ">   N20\n",
    ">   N30\n",
    ">   N40\n",
    ">   N50\n",
    ">   gc_content\n",
    ">   longest\n",
    ">   mean\n",
    ">   median\n",
    ">   sequence_count\n",
    ">   shortest\n",
    ">   total_bps\n",
    ">   \n",
    "> \"\"\"\n",
    ">\n",
    "> ctig_len = strint(df, \"Contigs\", \"N50\")\n",
    ">\n",
    "> # -->> Looking inside the strint(dataframe, category, label) function ---->>\n",
    ">\n",
    "> # Which then finds the desired value or 'Value'\n",
    "> stat = dataframe.loc[(category, label), \"Value\"]\n",
    "> ```\n",
    "\n",
    "### Dictionary Input"
   ],
   "id": "f011610e-c409-48e7-88b0-1f3dcc4be70d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Contigs\n",
    "ctig_len = strint(statsDict, \"Contig Stats\", \"total_bps\")\n",
    "ctig_count = strint(statsDict, \"Contig Stats\", \"sequence_count\")\n",
    "ctig_n50 = strint(statsDict, \"Contig Stats\", \"N50\")\n",
    "ctig_max = strint(statsDict, \"Contig Stats\", \"longest\")\n",
    "\n",
    "#---- Scaffolds\n",
    "sfld_len = strint(statsDict, \"Scaffold Stats\", \"total_bps\")\n",
    "sfld_count = strint(statsDict, \"Scaffold Stats\", \"sequence_count\")\n",
    "sfld_n50 = strint(statsDict, \"Scaffold Stats\", \"N50\")\n",
    "sfld_max = strint(statsDict, \"Scaffold Stats\", \"longest\")"
   ],
   "id": "65f7c5d3-001e-401b-b525-c01738af609f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dictionary query syntax**\n",
    ">\n",
    "> ``` python\n",
    "> \"\"\"\n",
    ">\n",
    "> strint(dataframe, category, label)\n",
    ">\n",
    "> category options\n",
    "> ----------------\n",
    ">   Contig Stats\n",
    ">   Scaffold Stats\n",
    ">\n",
    ">\n",
    "> label options\n",
    "> ----------------\n",
    ">   L10\n",
    ">   L20\n",
    ">   L30\n",
    ">   L40\n",
    ">   L50\n",
    ">   N10\n",
    ">   N20\n",
    ">   N30\n",
    ">   N40\n",
    ">   N50\n",
    ">   gc_content\n",
    ">   longest\n",
    ">   mean\n",
    ">   median\n",
    ">   sequence_count\n",
    ">   shortest\n",
    ">   total_bps\n",
    ">   \n",
    "> \"\"\"\n",
    ">\n",
    "> # A look inside assemblyDictionary.py where stat_output is returned\n",
    "> stat_output = {'Contig Stats': contig_stats,\n",
    ">                'Scaffold Stats': scaffold_stats}\n",
    ">\n",
    "> ctig_len = statsDict[\"Contig Stats\"][\"total_bps\"]\n",
    "> ```\n",
    "\n",
    "## Present\n",
    "\n",
    "### The Pandas Table\n",
    "\n",
    "This is a slightly formatted view of the Pandas table designed to be more easily queried to return the desired statistic. If, however, you’d like to treat the Styler object as the unchanged, dataframe object, use the `forma_df.data` syntax.\n",
    "\n",
    "[:The original dataframe output:](#NutFrame)"
   ],
   "id": "fe1e556d-457d-4b80-9b07-ccc140ee5e89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n"
      ],
      "text/markdown": [
       "  ----------- ---------------- --------------------\n",
       "  Contigs     L10              41\n",
       "              L20              99\n",
       "              L30              174\n",
       "              L40              267\n",
       "              L50              385\n",
       "              N10              12643769\n",
       "              N20              9681846\n",
       "              N30              7895799\n",
       "              N40              6288966\n",
       "              N50              4902968\n",
       "              gc_content       35.248103813419206\n",
       "              longest          43529772\n",
       "              mean             1552486.9914285715\n",
       "              median           331935.0\n",
       "              sequence_count   4200\n",
       "              shortest         1000\n",
       "              total_bps        6520445364\n",
       "  Scaffolds   L10              0\n",
       "              L20              0\n",
       "              L30              1\n",
       "              L40              2\n",
       "              L50              3\n",
       "              N10              1438277616\n",
       "              N20              1438277616\n",
       "              N30              915491830\n",
       "              N40              607508155\n",
       "              N50              518932092\n",
       "              gc_content       35.248103813419206\n",
       "              longest          1438277616\n",
       "              mean             3212576.533990148\n",
       "              median           62362.5\n",
       "              sequence_count   2030\n",
       "              shortest         1000\n",
       "              total_bps        6521530364\n",
       "  ----------- ---------------- --------------------\n"
      ]
     }
    }
   ],
   "source": [
    "# Display with Style \n",
    "forma_df = df.loc[:].style.pipe(formaFrame)\n",
    "\n",
    "forma_df"
   ],
   "id": "c69d4a94-0044-40a1-9bed-bc94ffe9ec4d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNA Zoo’s Table, Reproduced"
   ],
   "id": "9cac02ab-cb9b-41f4-bb61-4b7e5ab298cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(reticulate)"
   ],
   "id": "fc45a3d4-198e-44a4-9ca6-260bcd772acb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the library makes it simpler for inserting the values into the table below. For example, I would have had to type `{r} reticulate::py$ctig_n50`, but now, I only need to type `{r} py$ctig_n50` into the individual cells. I needed to convert the Python into R objects, as the knitr engine used in rendering this document does not seem to display output from execution of inline Python code directly.\n",
    "\n",
    "|  |  |  |  |\n",
    "|------------------|------------------|------------------|-------------------|\n",
    "| **Contig length (bp)** | **Number of contigs** | **Contig N50 (bp)** | **Longest contig (bp)** |\n",
    "| 6,520,445,364 | 4,200 | 4,902,968 | 43,529,772 |\n",
    "| **Scaffold length (bp)** | **Number of scaffolds** | **Scaffold N50 (bp)** | **Longest scaffold (bp)** |\n",
    "| 6,521,530,364 | 2,030 | 518,932,092 | 1,438,277,616 |\n",
    "\n",
    "## :x NutFrame"
   ],
   "id": "63a86c21-04c7-4417-9cfd-803e99fa6798"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                       Value\n",
      "Category  Label                             \n",
      "Contigs   L10                             41\n",
      "          L20                             99\n",
      "          L30                            174\n",
      "          L40                            267\n",
      "          L50                            385\n",
      "          N10                       12643769\n",
      "          N20                        9681846\n",
      "          N30                        7895799\n",
      "          N40                        6288966\n",
      "          N50                        4902968\n",
      "          gc_content      35.248103813419206\n",
      "          longest                   43529772\n",
      "          mean            1552486.9914285715\n",
      "          median                    331935.0\n",
      "          sequence_count                4200\n",
      "          shortest                      1000\n",
      "          total_bps               6520445364\n",
      "Scaffolds L10                              0\n",
      "          L20                              0\n",
      "          L30                              1\n",
      "          L40                              2\n",
      "          L50                              3\n",
      "          N10                     1438277616\n",
      "          N20                     1438277616\n",
      "          N30                      915491830\n",
      "          N40                      607508155\n",
      "          N50                      518932092\n",
      "          gc_content      35.248103813419206\n",
      "          longest                 1438277616\n",
      "          mean             3212576.533990148\n",
      "          median                     62362.5\n",
      "          sequence_count                2030\n",
      "          shortest                      1000\n",
      "          total_bps               6521530364"
     ]
    }
   ],
   "source": [
    "# The un-styled dataframe output\n",
    "forma_df.data"
   ],
   "id": "2a4076db-c6e6-42b5-8143-ee3c030e5374"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
